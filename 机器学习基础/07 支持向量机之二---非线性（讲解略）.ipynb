{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "统计学习方法，李航博士\n",
    "\n",
    "https://cuijiahua.com/blog/2017/11/ml_9_svm_2.html\n",
    "\n",
    "上篇文章讲解的是线性SVM的推导过程以及简化版SMO算法的代码实现。本篇文章将讲解SMO算法的优化方法以及非线性SVM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMO算法优化\n",
    "在几百个点组成的小规模数据集上，简化版SMO算法的运行是没有什么问题的，但是在更大的数据集上的运行速度就会变慢。简化版SMO算法的第二个α的选择是随机的，针对这一问题，我们可以使用启发式选择第二个α值，来达到优化效果。\n",
    "## 启发选择方式\n",
    "下面这两个公式想必已经不再陌生：![jupyter](./img/svm7-1.png)在实现SMO算法的时候，先计算η，再更新αj。为了加快第二个αj乘子的迭代速度，需要让直线的斜率增大，对于αj的更新公式，其中η值没有什么文章可做，于是只能令:$max|E_i-E_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们可以明确自己的优化方法了：\n",
    "1)最外层循环，首先在样本中选择违反KKT条件的一个乘子作为最外层循环，然后用\"启发式选择\"选择另外一个乘子并进行这两个乘子的优化\n",
    "2)在非边界乘子中寻找使得 |Ei - Ej| 最大的样本\n",
    "如果没有找到，则从整个样本中随机选择一个样本\n",
    "接下来，让我们看看完整版SMO算法如何实现。\n",
    "## 完整版SMO算法\n",
    "完整版Platt SMO算法是通过一个外循环来选择违反KKT条件的一个乘子，并且其选择过程会在这两种方式之间进行交替：\n",
    "1)在所有数据集上进行单遍扫描\n",
    "2)在非边界α中实现单遍扫描\n",
    "非边界α指的就是那些不等于边界0或C的α值，并且跳过那些已知的不会改变的α值。所以我们要先建立这些α的列表，用于才能出α的更新状态。\n",
    "在选择第一个α值后，算法会通过\"启发选择方式\"选择第二个α值。\n",
    "## 编写代码\n",
    "我们首先构建一个仅包含init方法的optStruct类，将其作为一个数据结构来使用，方便我们对于重要数据的维护。代码思路和之前的简化版SMO算法是相似的，不同之处在于增加了优化方法，如果上篇文章已经看懂，我想这个代码会很好理解。创建一个svm-smo.py文件，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L==H\n",
      "全样本遍历:第0次迭代 样本:0, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:1, alpha优化次数:0\n",
      "全样本遍历:第0次迭代 样本:2, alpha优化次数:1\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:3, alpha优化次数:1\n",
      "全样本遍历:第0次迭代 样本:4, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:5, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:6, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:7, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:8, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:9, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:10, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:11, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:12, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:13, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:14, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:15, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:16, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:17, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:18, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:19, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:20, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:21, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:22, alpha优化次数:3\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:23, alpha优化次数:3\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:24, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:25, alpha优化次数:4\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:26, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:27, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:28, alpha优化次数:4\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:29, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:30, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:31, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:32, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:33, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:34, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:35, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:36, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:37, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:38, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:39, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:40, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:41, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:42, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:43, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:44, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:45, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:46, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:47, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:48, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:49, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:50, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:51, alpha优化次数:5\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:52, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:53, alpha优化次数:5\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:54, alpha优化次数:5\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:55, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:56, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:57, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:58, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:59, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:60, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:61, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:62, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:63, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:64, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:65, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:66, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:67, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:68, alpha优化次数:5\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:69, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:70, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:71, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:72, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:73, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:74, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:75, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:76, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:77, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:78, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:79, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:80, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:81, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:82, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:83, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:84, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:85, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:86, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:87, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:88, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:89, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:90, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:91, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:92, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:93, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:94, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:95, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:96, alpha优化次数:6\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:97, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:98, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:99, alpha优化次数:6\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:0, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:3, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:4, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:17, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:18, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:25, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:46, alpha优化次数:0\n",
      "非边界遍历:第1次迭代 样本:55, alpha优化次数:0\n",
      "非边界遍历:第1次迭代 样本:94, alpha优化次数:0\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:0, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:1, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:2, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:3, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:4, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:5, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:6, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:7, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:8, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:9, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:10, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:11, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:12, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:13, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:14, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:15, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:16, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:17, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:18, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:19, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:20, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:21, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:22, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第2次迭代 样本:23, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:24, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:25, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:26, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:27, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:28, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第2次迭代 样本:29, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:30, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:31, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:32, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:33, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:34, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:35, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:36, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:37, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:38, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:39, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:40, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:41, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:42, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:43, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:44, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:45, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:46, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:47, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:48, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:49, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:50, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:51, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第2次迭代 样本:52, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:53, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第2次迭代 样本:54, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:55, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:56, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:57, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:58, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:59, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:60, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:61, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:62, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:63, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:64, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:65, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:66, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:67, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:68, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:69, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:70, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:71, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:72, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:73, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:74, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:75, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:76, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:77, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:78, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:79, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:80, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:81, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:82, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:83, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:84, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:85, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:86, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:87, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:88, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:89, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:90, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:91, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:92, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:93, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:94, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:95, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:96, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第2次迭代 样本:97, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:98, alpha优化次数:0\n",
      "全样本遍历:第2次迭代 样本:99, alpha优化次数:0\n",
      "迭代次数: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#在下面添加此代码以显示单元格中的所有输出\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "class optStruct:\n",
    "    \"\"\"\n",
    "    数据结构，维护所有需要操作的值\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "    \"\"\"\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler):\n",
    "        self.X = dataMatIn                                #数据矩阵\n",
    "        self.labelMat = classLabels                        #数据标签\n",
    "        self.C = C                                         #松弛变量\n",
    "        self.tol = toler                                 #容错率\n",
    "        self.m = np.shape(dataMatIn)[0]                 #数据矩阵行数\n",
    "        self.alphas = np.mat(np.zeros((self.m,1)))         #根据矩阵行数初始化alpha参数为0   \n",
    "        self.b = 0                                         #初始化b参数为0\n",
    "        self.eCache = np.mat(np.zeros((self.m,2)))         #根据矩阵行数初始化虎误差缓存，第一列为是否有效的标志位，第二列为实际的误差E的值。\n",
    " \n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    Parameters:\n",
    "        fileName - 文件名\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    " \n",
    "def calcEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算误差\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据\n",
    "    Returns:\n",
    "        Ek - 标号为k的数据误差\n",
    "    \"\"\"\n",
    "    fXk = float(np.multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T) + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    " \n",
    "def selectJrand(i, m):\n",
    "    \"\"\"\n",
    "    函数说明:随机选择alpha_j的索引值\n",
    " \n",
    "    Parameters:\n",
    "        i - alpha_i的索引值\n",
    "        m - alpha参数个数\n",
    "    Returns:\n",
    "        j - alpha_j的索引值\n",
    "    \"\"\"\n",
    "    j = i                                 #选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    " \n",
    "def selectJ(i, oS, Ei):\n",
    "    \"\"\"\n",
    "    内循环启发方式2\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "        Ei - 标号为i的数据误差\n",
    "    Returns:\n",
    "        j, maxK - 标号为j或maxK的数据的索引值\n",
    "        Ej - 标号为j的数据误差\n",
    "    \"\"\"\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0                         #初始化\n",
    "    oS.eCache[i] = [1,Ei]                                      #根据Ei更新误差缓存\n",
    "    validEcacheList = np.nonzero(oS.eCache[:,0].A)[0]        #返回误差不为0的数据的索引值\n",
    "    if (len(validEcacheList)) > 1:                            #有不为0的误差\n",
    "        for k in validEcacheList:                           #遍历,找到最大的Ek\n",
    "            if k == i: continue                             #不计算i,浪费时间\n",
    "            Ek = calcEk(oS, k)                                #计算Ek\n",
    "            deltaE = abs(Ei - Ek)                            #计算|Ei-Ek|\n",
    "            if (deltaE > maxDeltaE):                        #找到maxDeltaE\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej                                        #返回maxK,Ej\n",
    "    else:                                                   #没有不为0的误差\n",
    "        j = selectJrand(i, oS.m)                            #随机选择alpha_j的索引值\n",
    "        Ej = calcEk(oS, j)                                    #计算Ej\n",
    "    return j, Ej                                             #j,Ej\n",
    " \n",
    "def updateEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算Ek,并更新误差缓存\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据的索引值\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    Ek = calcEk(oS, k)                                        #计算Ek\n",
    "    oS.eCache[k] = [1,Ek]                                    #更新误差缓存\n",
    " \n",
    " \n",
    "def clipAlpha(aj,H,L):\n",
    "    \"\"\"\n",
    "    修剪alpha_j\n",
    "    Parameters:\n",
    "        aj - alpha_j的值\n",
    "        H - alpha上限\n",
    "        L - alpha下限\n",
    "    Returns:\n",
    "        aj - 修剪后的alpah_j的值\n",
    "    \"\"\"\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    " \n",
    "def innerL(i, oS):\n",
    "    \"\"\"\n",
    "    优化的SMO算法\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "    Returns:\n",
    "        1 - 有任意一对alpha值发生变化\n",
    "        0 - 没有任意一对alpha值发生变化或变化太小\n",
    "    \"\"\"\n",
    "    #步骤1：计算误差Ei\n",
    "    Ei = calcEk(oS, i)\n",
    "    #优化alpha,设定一定的容错率。\n",
    "    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        #使用内循环启发方式2选择alpha_j,并计算Ej\n",
    "        j,Ej = selectJ(i, oS, Ei)\n",
    "        #保存更新前的aplpha值，使用深拷贝\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        #步骤2：计算上下界L和H\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "            print(\"L==H\")\n",
    "            return 0\n",
    "        #步骤3：计算eta\n",
    "        eta = 2.0 * oS.X[i,:] * oS.X[j,:].T - oS.X[i,:] * oS.X[i,:].T - oS.X[j,:] * oS.X[j,:].T\n",
    "        if eta >= 0:\n",
    "            print(\"eta>=0\")\n",
    "            return 0\n",
    "        #步骤4：更新alpha_j\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej)/eta\n",
    "        #步骤5：修剪alpha_j\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        #更新Ej至误差缓存\n",
    "        updateEk(oS, j)\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n",
    "            print(\"alpha_j变化太小\")\n",
    "            return 0\n",
    "        #步骤6：更新alpha_i\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])\n",
    "        #更新Ei至误差缓存\n",
    "        updateEk(oS, i)\n",
    "        #步骤7：更新b_1和b_2\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[i,:]*oS.X[j,:].T\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[j,:]*oS.X[j,:].T\n",
    "        #步骤8：根据b_1和b_2更新b\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    \"\"\"\n",
    "    完整的线性SMO算法\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        maxIter - 最大迭代次数\n",
    "    Returns:\n",
    "        oS.b - SMO算法计算的b\n",
    "        oS.alphas - SMO算法计算的alphas\n",
    "    \"\"\"\n",
    "    oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler)                    #初始化数据结构\n",
    "    iter = 0                                                                                         #初始化当前迭代次数\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):                            #遍历整个数据集都alpha也没有更新或者超过最大迭代次数,则退出循环\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                                                                                #遍历整个数据集                           \n",
    "            for i in range(oS.m):       \n",
    "                alphaPairsChanged += innerL(i,oS)                                                    #使用优化的SMO算法\n",
    "                print(\"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                                                                                         #遍历非边界值\n",
    "            nonBoundIs = np.nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]                        #遍历不在边界0和C的alpha\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print(\"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:                                                                                #遍历一次后改为非边界遍历\n",
    "            entireSet = False\n",
    "        elif (alphaPairsChanged == 0):                                                                #如果alpha没有更新,计算全样本遍历\n",
    "            entireSet = True \n",
    "        print(\"迭代次数: %d\" % iter)\n",
    "    return oS.b,oS.alphas                                                                             #返回SMO算法计算的b和alphas\n",
    " \n",
    " \n",
    "def showClassifer(dataMat, classLabels, w, b):\n",
    "    \"\"\"\n",
    "    分类结果可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        w - 直线法向量\n",
    "        b - 直线解决\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    #绘制样本点\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if classLabels[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1], s=30, alpha=0.7)   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1], s=30, alpha=0.7) #负样本散点图\n",
    "    #绘制直线\n",
    "    x1 = max(dataMat)[0]\n",
    "    x2 = min(dataMat)[0]\n",
    "    a1, a2 = w\n",
    "    b = float(b)\n",
    "    a1 = float(a1[0])\n",
    "    a2 = float(a2[0])\n",
    "    y1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2\n",
    "    plt.plot([x1, x2], [y1, y2])\n",
    "    #找出支持向量点\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if abs(alpha) > 0:\n",
    "            x, y = dataMat[i]\n",
    "            plt.scatter([x], [y], s=150, c='none', alpha=0.7, linewidth=1.5, edgecolor='red')\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "def calcWs(alphas,dataArr,classLabels):\n",
    "    \"\"\"\n",
    "    计算w\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        alphas - alphas值\n",
    "    Returns:\n",
    "        w - 计算得到的w\n",
    "    \"\"\"\n",
    "    X = np.mat(dataArr); labelMat = np.mat(classLabels).transpose()\n",
    "    m,n = np.shape(X)\n",
    "    w = np.zeros((n,1))\n",
    "    for i in range(m):\n",
    "        w += np.multiply(alphas[i]*labelMat[i],X[i,:].T)\n",
    "    return w\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataArr, classLabels = loadDataSet('./DataSet/SvmtestSet2.txt')\n",
    "    b, alphas = smoP(dataArr, classLabels, 0.6, 0.001, 40)\n",
    "    w = calcWs(alphas,dataArr, classLabels)\n",
    "    showClassifer(dataArr, classLabels, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中画红圈的样本点为支持向量上的点，是满足算法的一种解。完整版SMO算法覆盖整个数据集进行计算，而简化版SMO算法是随机选择的。可以看出，完整版SMO算法选出的支持向量样点更多，更接近理想的分隔超平面。![jupyter](./img/svm7-2.png)\n",
    "\n",
    "对比两种算法的运算时间，我的测试结果是完整版SMO算法的速度比简化版SMO算法的速度快6倍左右。\n",
    "\n",
    "其实，优化方法不仅仅是简单的启发式选择，还有其他优化方法，SMO算法速度还可以进一步提高。但是鉴于文章进度，这里不再进行展开。\n",
    "\n",
    "# 非线性SVM\n",
    "## 核技巧\n",
    "我们已经了解到，SVM如何处理线性可分的情况，而对于非线性的情况，SVM的处理方式就是选择一个核函数。简而言之：在线性不可分的情况下，SVM通过某种事先选择的**非线性映射（核函数）**将输入变量映到一个高维特征空间，将其变成在高维空间线性可分，在这个高维空间中构造最优分类超平面。\n",
    "根据上篇文章，线性可分的情况下，可知最终的超平面方程为：$$f(x)=\\sum_{i=1}^na_iy_ix_i^Tx+b$$用内积表示$$f(x)=\\sum_{i=1}^na_iy_i<x_i,x>+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性不可分，我们使用一个非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，分类函数变形如下：![jupyter](./img/svm7-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中ϕ从输入空间(X)到某个特征空间(F)的映射，这意味着建立非线性学习器分为两步：\n",
    "\n",
    "1)首先**使用一个非线性映射将数据变换到一个特征空间F**；\n",
    "\n",
    "2)然后**在特征空间使用线性学习器分类**。\n",
    "\n",
    "如果有一种方法可以在特征空间中直接计算内积 <ϕ(xi),ϕ(x)>，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个分线性的学习器，这样直接计算的方法称为**核函数方法**。\n",
    "\n",
    "这里直接给出一个定义：**核是一个函数k，对所有x,z∈X，满足k(x,z)=<ϕ(xi),ϕ(x)>，这里ϕ(·)是从原始输入空间X到内积空间F的映射**。\n",
    "\n",
    "简而言之：如果不是用核技术，就会先计算线性映ϕ(x1)和ϕ(x2)，然后计算它们的内积，使用了核技术之后，先把ϕ(x1)和ϕ(x2)的一般表达式<ϕ(x1),ϕ(x2)>=k(<ϕ(x1),ϕ(x2) >)计算出来，这里的<·，·>表示内积，k(·，·)就是对应的核函数，这个表达式往往非常简单，所以计算非常方便。\n",
    "\n",
    "这种将内积替换成核函数的方式被称为**核技巧(kernel trick)**。\n",
    "\n",
    "## 非线性数据处理\n",
    "已经知道了核技巧是什么，但是为什么要这样做呢？我们先举一个简单的例子，进行说明。假设二维平面x-y上存在若干点，其中点集A服从$ {x,y|x^2+y^2=1}$，点集B服从${x,y|x^2+y^2=9}$，那么这些点在二维平面上的分布是这样的： ![jupyter](./img/svm7-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝色的是点集A，红色的是点集B，他们在xy平面上并不能线性可分，即用一条直线分割（ 虽然肉眼是可以识别的） 。采用映射$\\{(x,y)->(x,y,x^2+y^2)\\}$后，在三维空间的点的分布为：![jupyter](./img/svm7-5.png)红色和蓝色的点被映射到了不同的平面，在更高维空间中是线性可分的（用一个平面去分割）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核技巧的实现\n",
    "通过核技巧的转变，我们的分类函数变为：![jupyter](./img/svm7-6.png)对偶问题变成了：![jupyter](./im./svm7-7.png)\n",
    "这样，我们就避开了高维度空间中的计算。当然，我们刚刚的例子是非常简单的，我们可以手动构造出来对应映射的核函数出来，如果对于任意一个映射，要构造出对应的核函数就很困难了。因此，通常，**人们会从一些常用的核函数中进行选择**，根据问题和数据的不同，选择不同的参数，得到不同的核函数。接下来，要介绍的就是一个非常流行的核函数，那就是**径向基核函数**。\n",
    "\n",
    "径向基核函数是SVM中常用的一个核函数。径向基核函数采用向量作为自变量的函数，能够基于向量举例运算输出一个标量。径向基核函数的高斯版本的公式如下：\n",
    "![jupyter](./img/svm7-8.png)其中，\n",
    "\n",
    "σ是用户自定义的用于确定到达率(reach)或者说函数值跌落到0的速度参数。\n",
    "\n",
    "上述高斯核函数将数据从原始空间映射到无穷维空间。关于无穷维空间，我们不必太担心。高斯核函数只是一个常用的核函数，使用者并不需要确切地理解数据到底是如何表现的，而且使用高斯核函数还会得到一个理想的结果。\n",
    "\n",
    "如果σ选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果σ选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数σ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编程实现非线性SVM\n",
    "接下来，我们将使用SVMtestSetRBF.txt和SVMtestSetRBF2.txt，前者作为训练集，后者作为测试集。\n",
    "## 可视化数据集\n",
    "我们先编写程序简单看下数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './DSVMtestSetRBF.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a065440a4171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdataArr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./DSVMtestSetRBF.txt'\u001b[0m\u001b[1;33m)\u001b[0m                        \u001b[1;31m#加载训练集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mshowDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e9be281f024f>\u001b[0m in \u001b[0;36mloadDataSet\u001b[1;34m(fileName)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m     38\u001b[0m     \u001b[0mdataMat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mlabelMat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m                                     \u001b[1;31m#逐行读取，滤除空格等\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mlineArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './DSVMtestSetRBF.txt'"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #负样本散点图\n",
    "    plt.show()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataArr,labelArr = loadDataSet('./DSVMtestSetRBF.txt')                        #加载训练集\n",
    "    showDataSet(dataArr, labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，数据明显是线性不可分的。下面我们根据公式，编写核函数，并增加初始化参数kTup用于存储核函数有关的信息，同时我们只要将之前的内积运算变成核函数的运算即可。最后编写testRbf()函数，用于测试。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    " \n",
    "class optStruct:\n",
    "    \"\"\"\n",
    "    数据结构，维护所有需要操作的值\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        kTup - 包含核函数信息的元组,第一个参数存放核函数类别，第二个参数存放必要的核函数需要用到的参数\n",
    "    \"\"\"\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n",
    "        self.X = dataMatIn                                #数据矩阵\n",
    "        self.labelMat = classLabels                        #数据标签\n",
    "        self.C = C                                         #松弛变量\n",
    "        self.tol = toler                                 #容错率\n",
    "        self.m = np.shape(dataMatIn)[0]                 #数据矩阵行数\n",
    "        self.alphas = np.mat(np.zeros((self.m,1)))         #根据矩阵行数初始化alpha参数为0   \n",
    "        self.b = 0                                         #初始化b参数为0\n",
    "        self.eCache = np.mat(np.zeros((self.m,2)))         #根据矩阵行数初始化虎误差缓存，第一列为是否有效的标志位，第二列为实际的误差E的值。\n",
    "        self.K = np.mat(np.zeros((self.m,self.m)))        #初始化核K\n",
    "        for i in range(self.m):                            #计算所有数据的核K\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n",
    " \n",
    "def kernelTrans(X, A, kTup):\n",
    "    \"\"\"\n",
    "    通过核函数将数据转换更高维的空间\n",
    "    Parameters：\n",
    "        X - 数据矩阵\n",
    "        A - 单个数据的向量\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        K - 计算的核K\n",
    "    \"\"\"\n",
    "    m,n = np.shape(X)\n",
    "    K = np.mat(np.zeros((m,1)))\n",
    "    if kTup[0] == 'lin': K = X * A.T                       #线性核函数,只进行内积。\n",
    "    elif kTup[0] == 'rbf':                                 #高斯核函数,根据高斯核函数公式进行计算\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow*deltaRow.T\n",
    "        K = np.exp(K/(-1*kTup[1]**2))                     #计算高斯核K\n",
    "    else: raise NameError('核函数无法识别')\n",
    "    return K                                             #返回计算的核K\n",
    " \n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    Parameters:\n",
    "        fileName - 文件名\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    " \n",
    "def calcEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算误差\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据\n",
    "    Returns:\n",
    "        Ek - 标号为k的数据误差\n",
    "    \"\"\"\n",
    "    fXk = float(np.multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    " \n",
    "def selectJrand(i, m):\n",
    "    \"\"\"\n",
    "    函数说明:随机选择alpha_j的索引值\n",
    " \n",
    "    Parameters:\n",
    "        i - alpha_i的索引值\n",
    "        m - alpha参数个数\n",
    "    Returns:\n",
    "        j - alpha_j的索引值\n",
    "    \"\"\"\n",
    "    j = i                                 #选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    " \n",
    "def selectJ(i, oS, Ei):\n",
    "    \"\"\"\n",
    "    内循环启发方式2\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "        Ei - 标号为i的数据误差\n",
    "    Returns:\n",
    "        j, maxK - 标号为j或maxK的数据的索引值\n",
    "        Ej - 标号为j的数据误差\n",
    "    \"\"\"\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0                         #初始化\n",
    "    oS.eCache[i] = [1,Ei]                                      #根据Ei更新误差缓存\n",
    "    validEcacheList = np.nonzero(oS.eCache[:,0].A)[0]        #返回误差不为0的数据的索引值\n",
    "    if (len(validEcacheList)) > 1:                            #有不为0的误差\n",
    "        for k in validEcacheList:                           #遍历,找到最大的Ek\n",
    "            if k == i: continue                             #不计算i,浪费时间\n",
    "            Ek = calcEk(oS, k)                                #计算Ek\n",
    "            deltaE = abs(Ei - Ek)                            #计算|Ei-Ek|\n",
    "            if (deltaE > maxDeltaE):                        #找到maxDeltaE\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej                                        #返回maxK,Ej\n",
    "    else:                                                   #没有不为0的误差\n",
    "        j = selectJrand(i, oS.m)                            #随机选择alpha_j的索引值\n",
    "        Ej = calcEk(oS, j)                                    #计算Ej\n",
    "    return j, Ej                                             #j,Ej\n",
    " \n",
    "def updateEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算Ek,并更新误差缓存\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据的索引值\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    Ek = calcEk(oS, k)                                        #计算Ek\n",
    "    oS.eCache[k] = [1,Ek]                                    #更新误差缓存\n",
    " \n",
    "def clipAlpha(aj,H,L):\n",
    "    \"\"\"\n",
    "    修剪alpha_j\n",
    "    Parameters:\n",
    "        aj - alpha_j的值\n",
    "        H - alpha上限\n",
    "        L - alpha下限\n",
    "    Returns:\n",
    "        aj - 修剪后的alpah_j的值\n",
    "    \"\"\"\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    " \n",
    "def innerL(i, oS):\n",
    "    \"\"\"\n",
    "    优化的SMO算法\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "    Returns:\n",
    "        1 - 有任意一对alpha值发生变化\n",
    "        0 - 没有任意一对alpha值发生变化或变化太小\n",
    "    \"\"\"\n",
    "    #步骤1：计算误差Ei\n",
    "    Ei = calcEk(oS, i)\n",
    "    #优化alpha,设定一定的容错率。\n",
    "    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        #使用内循环启发方式2选择alpha_j,并计算Ej\n",
    "        j,Ej = selectJ(i, oS, Ei)\n",
    "        #保存更新前的aplpha值，使用深拷贝\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        #步骤2：计算上下界L和H\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "            print(\"L==H\")\n",
    "            return 0\n",
    "        #步骤3：计算eta\n",
    "        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j]\n",
    "        if eta >= 0:\n",
    "            print(\"eta>=0\")\n",
    "            return 0\n",
    "        #步骤4：更新alpha_j\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej)/eta\n",
    "        #步骤5：修剪alpha_j\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        #更新Ej至误差缓存\n",
    "        updateEk(oS, j)\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n",
    "            print(\"alpha_j变化太小\")\n",
    "            return 0\n",
    "        #步骤6：更新alpha_i\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])\n",
    "        #更新Ei至误差缓存\n",
    "        updateEk(oS, i)\n",
    "        #步骤7：更新b_1和b_2\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n",
    "        #步骤8：根据b_1和b_2更新b\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup = ('lin',0)):\n",
    "    \"\"\"\n",
    "    完整的线性SMO算法\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        maxIter - 最大迭代次数\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        oS.b - SMO算法计算的b\n",
    "        oS.alphas - SMO算法计算的alphas\n",
    "    \"\"\"\n",
    "    oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler, kTup)                #初始化数据结构\n",
    "    iter = 0                                                                                         #初始化当前迭代次数\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):                            #遍历整个数据集都alpha也没有更新或者超过最大迭代次数,则退出循环\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                                                                                #遍历整个数据集                           \n",
    "            for i in range(oS.m):       \n",
    "                alphaPairsChanged += innerL(i,oS)                                                    #使用优化的SMO算法\n",
    "                print(\"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                                                                                         #遍历非边界值\n",
    "            nonBoundIs = np.nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]                        #遍历不在边界0和C的alpha\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print(\"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:                                                                                #遍历一次后改为非边界遍历\n",
    "            entireSet = False\n",
    "        elif (alphaPairsChanged == 0):                                                                #如果alpha没有更新,计算全样本遍历\n",
    "            entireSet = True \n",
    "        print(\"迭代次数: %d\" % iter)\n",
    "    return oS.b,oS.alphas                                                                             #返回SMO算法计算的b和alphas\n",
    " \n",
    "def testRbf(k1 = 1.3):\n",
    "    \"\"\"\n",
    "    测试函数\n",
    "    Parameters:\n",
    "        k1 - 使用高斯核函数的时候表示到达率\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    dataArr,labelArr = loadDataSet('./DataSet/SVMtestSetRBF.txt')                        #加载训练集\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 100, ('rbf', k1))        #根据训练集计算b和alphas\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()\n",
    "    svInd = np.nonzero(alphas.A > 0)[0]                                        #获得支持向量\n",
    "    sVs = datMat[svInd]                                                     \n",
    "    labelSV = labelMat[svInd];\n",
    "    print(\"支持向量个数:%d\" % np.shape(sVs)[0])\n",
    "    m,n = np.shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))                #计算各个点的核\n",
    "        predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b     #根据支持向量的点，计算超平面，返回预测结果\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1        #返回数组中各元素的正负符号，用1和-1表示，并统计错误个数\n",
    "    print(\"训练集错误率: %.2f%%\" % ((float(errorCount)/m)*100))             #打印错误率\n",
    "    dataArr,labelArr = loadDataSet('SVMtestSetRBF2.txt')                         #加载测试集\n",
    "    errorCount = 0\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()         \n",
    "    m,n = np.shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))                 #计算各个点的核           \n",
    "        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b         #根据支持向量的点，计算超平面，返回预测结果\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1        #返回数组中各元素的正负符号，用1和-1表示，并统计错误个数\n",
    "    print(\"测试集错误率: %.2f%%\" % ((float(errorCount)/m)*100))             #打印错误率\n",
    " \n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #负样本散点图\n",
    "    plt.show()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    testRbf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，训练集错误率为1%，测试集错误率都是4%，训练耗时1.7s 。可以尝试更换不同的K1参数以观察测试错误率、训练错误率、支持向量个数随k1的变化情况。你会发现K1过大，会出现过拟合的情况，即训练集错误率低，但是测试集错误率高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn构建SVM分类器\n",
    "我们使用了kNN进行手写数字识别。它的缺点是存储空间大。现在我们使用SVM，因为它只需要保留支持向量即可，可以节省空间，而且能获得可比的效果。\n",
    "\n",
    "使用的数据集还是kNN用到的数据集（testDigits和trainingDigits）。\n",
    "\n",
    "首先，我们先使用自己用python写的代码进行训练。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class optStruct:\n",
    "    \"\"\"\n",
    "    数据结构，维护所有需要操作的值\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        kTup - 包含核函数信息的元组,第一个参数存放核函数类别，第二个参数存放必要的核函数需要用到的参数\n",
    "    \"\"\"\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n",
    "        self.X = dataMatIn                                #数据矩阵\n",
    "        self.labelMat = classLabels                        #数据标签\n",
    "        self.C = C                                         #松弛变量\n",
    "        self.tol = toler                                 #容错率\n",
    "        self.m = np.shape(dataMatIn)[0]                 #数据矩阵行数\n",
    "        self.alphas = np.mat(np.zeros((self.m,1)))         #根据矩阵行数初始化alpha参数为0   \n",
    "        self.b = 0                                         #初始化b参数为0\n",
    "        self.eCache = np.mat(np.zeros((self.m,2)))         #根据矩阵行数初始化虎误差缓存，第一列为是否有效的标志位，第二列为实际的误差E的值。\n",
    "        self.K = np.mat(np.zeros((self.m,self.m)))        #初始化核K\n",
    "        for i in range(self.m):                            #计算所有数据的核K\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n",
    "\n",
    "def kernelTrans(X, A, kTup):\n",
    "    \"\"\"\n",
    "    通过核函数将数据转换更高维的空间\n",
    "    Parameters：\n",
    "        X - 数据矩阵\n",
    "        A - 单个数据的向量\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        K - 计算的核K\n",
    "    \"\"\"\n",
    "    m,n = np.shape(X)\n",
    "    K = np.mat(np.zeros((m,1)))\n",
    "    if kTup[0] == 'lin': K = X * A.T                       #线性核函数,只进行内积。\n",
    "    elif kTup[0] == 'rbf':                                 #高斯核函数,根据高斯核函数公式进行计算\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow*deltaRow.T\n",
    "        K = np.exp(K/(-1*kTup[1]**2))                     #计算高斯核K\n",
    "    else: raise NameError('核函数无法识别')\n",
    "    return K                                             #返回计算的核K\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    Parameters:\n",
    "        fileName - 文件名\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def calcEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算误差\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据\n",
    "    Returns:\n",
    "        Ek - 标号为k的数据误差\n",
    "    \"\"\"\n",
    "    fXk = float(np.multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    "\n",
    "def selectJrand(i, m):\n",
    "    \"\"\"\n",
    "    函数说明:随机选择alpha_j的索引值\n",
    "\n",
    "    Parameters:\n",
    "        i - alpha_i的索引值\n",
    "        m - alpha参数个数\n",
    "    Returns:\n",
    "        j - alpha_j的索引值\n",
    "    \"\"\"\n",
    "    j = i                                 #选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    "\n",
    "def selectJ(i, oS, Ei):\n",
    "    \"\"\"\n",
    "    内循环启发方式2\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "        Ei - 标号为i的数据误差\n",
    "    Returns:\n",
    "        j, maxK - 标号为j或maxK的数据的索引值\n",
    "        Ej - 标号为j的数据误差\n",
    "    \"\"\"\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0                         #初始化\n",
    "    oS.eCache[i] = [1,Ei]                                      #根据Ei更新误差缓存\n",
    "    validEcacheList = np.nonzero(oS.eCache[:,0].A)[0]        #返回误差不为0的数据的索引值\n",
    "    if (len(validEcacheList)) > 1:                            #有不为0的误差\n",
    "        for k in validEcacheList:                           #遍历,找到最大的Ek\n",
    "            if k == i: continue                             #不计算i,浪费时间\n",
    "            Ek = calcEk(oS, k)                                #计算Ek\n",
    "            deltaE = abs(Ei - Ek)                            #计算|Ei-Ek|\n",
    "            if (deltaE > maxDeltaE):                        #找到maxDeltaE\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej                                        #返回maxK,Ej\n",
    "    else:                                                   #没有不为0的误差\n",
    "        j = selectJrand(i, oS.m)                            #随机选择alpha_j的索引值\n",
    "        Ej = calcEk(oS, j)                                    #计算Ej\n",
    "    return j, Ej                                             #j,Ej\n",
    "\n",
    "def updateEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算Ek,并更新误差缓存\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据的索引值\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    Ek = calcEk(oS, k)                                        #计算Ek\n",
    "    oS.eCache[k] = [1,Ek]                                    #更新误差缓存\n",
    "\n",
    "\n",
    "def clipAlpha(aj,H,L):\n",
    "    \"\"\"\n",
    "    修剪alpha_j\n",
    "    Parameters:\n",
    "        aj - alpha_j的值\n",
    "        H - alpha上限\n",
    "        L - alpha下限\n",
    "    Returns:\n",
    "        aj - 修剪后的alpah_j的值\n",
    "    \"\"\"\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    "\n",
    "def innerL(i, oS):\n",
    "    \"\"\"\n",
    "    优化的SMO算法\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "    Returns:\n",
    "        1 - 有任意一对alpha值发生变化\n",
    "        0 - 没有任意一对alpha值发生变化或变化太小\n",
    "    \"\"\"\n",
    "    #步骤1：计算误差Ei\n",
    "    Ei = calcEk(oS, i)\n",
    "    #优化alpha,设定一定的容错率。\n",
    "    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        #使用内循环启发方式2选择alpha_j,并计算Ej\n",
    "        j,Ej = selectJ(i, oS, Ei)\n",
    "        #保存更新前的aplpha值，使用深拷贝\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        #步骤2：计算上下界L和H\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "            print(\"L==H\")\n",
    "            return 0\n",
    "        #步骤3：计算eta\n",
    "        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j]\n",
    "        if eta >= 0:\n",
    "            print(\"eta>=0\")\n",
    "            return 0\n",
    "        #步骤4：更新alpha_j\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej)/eta\n",
    "        #步骤5：修剪alpha_j\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        #更新Ej至误差缓存\n",
    "        updateEk(oS, j)\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n",
    "            print(\"alpha_j变化太小\")\n",
    "            return 0\n",
    "        #步骤6：更新alpha_i\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])\n",
    "        #更新Ei至误差缓存\n",
    "        updateEk(oS, i)\n",
    "        #步骤7：更新b_1和b_2\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n",
    "        #步骤8：根据b_1和b_2更新b\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup = ('lin',0)):\n",
    "    \"\"\"\n",
    "    完整的线性SMO算法\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        maxIter - 最大迭代次数\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        oS.b - SMO算法计算的b\n",
    "        oS.alphas - SMO算法计算的alphas\n",
    "    \"\"\"\n",
    "    oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler, kTup)                #初始化数据结构\n",
    "    iter = 0                                                                                         #初始化当前迭代次数\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):                            #遍历整个数据集都alpha也没有更新或者超过最大迭代次数,则退出循环\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                                                                                #遍历整个数据集                           \n",
    "            for i in range(oS.m):       \n",
    "                alphaPairsChanged += innerL(i,oS)                                                    #使用优化的SMO算法\n",
    "                print(\"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                                                                                         #遍历非边界值\n",
    "            nonBoundIs = np.nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]                        #遍历不在边界0和C的alpha\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print(\"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:                                                                                #遍历一次后改为非边界遍历\n",
    "            entireSet = False\n",
    "        elif (alphaPairsChanged == 0):                                                                #如果alpha没有更新,计算全样本遍历\n",
    "            entireSet = True \n",
    "        print(\"迭代次数: %d\" % iter)\n",
    "    return oS.b,oS.alphas                                                                             #返回SMO算法计算的b和alphas\n",
    "\n",
    "\n",
    "def img2vector(filename):\n",
    "    \"\"\"\n",
    "    将32x32的二进制图像转换为1x1024向量。\n",
    "    Parameters:\n",
    "        filename - 文件名\n",
    "    Returns:\n",
    "        returnVect - 返回的二进制图像的1x1024向量\n",
    "    \"\"\"\n",
    "    returnVect = np.zeros((1,1024))\n",
    "    fr = open(filename)\n",
    "    for i in range(32):\n",
    "        lineStr = fr.readline()\n",
    "        for j in range(32):\n",
    "            returnVect[0,32*i+j] = int(lineStr[j])\n",
    "    return returnVect\n",
    "\n",
    "def loadImages(dirName):\n",
    "    \"\"\"\n",
    "    加载图片\n",
    "    Parameters:\n",
    "        dirName - 文件夹的名字\n",
    "    Returns:\n",
    "        trainingMat - 数据矩阵\n",
    "        hwLabels - 数据标签\n",
    "    \"\"\"\n",
    "    from os import listdir\n",
    "    hwLabels = []\n",
    "    trainingFileList = listdir(dirName)           \n",
    "    m = len(trainingFileList)\n",
    "    trainingMat = np.zeros((m,1024))\n",
    "    for i in range(m):\n",
    "        fileNameStr = trainingFileList[i]\n",
    "        fileStr = fileNameStr.split('.')[0]     \n",
    "        classNumStr = int(fileStr.split('_')[0])\n",
    "        if classNumStr == 9: hwLabels.append(-1)\n",
    "        else: hwLabels.append(1)\n",
    "        trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileNameStr))\n",
    "    return trainingMat, hwLabels   \n",
    "\n",
    "def testDigits(kTup=('rbf', 10)):\n",
    "    \"\"\"\n",
    "    测试函数\n",
    "    Parameters:\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    dataArr,labelArr = loadImages('trainingDigits')\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10, kTup)\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()\n",
    "    svInd = np.nonzero(alphas.A>0)[0]\n",
    "    sVs=datMat[svInd]\n",
    "    labelSV = labelMat[svInd];\n",
    "    print(\"支持向量个数:%d\" % np.shape(sVs)[0])\n",
    "    m,n = np.shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1\n",
    "    print(\"训练集错误率: %.2f%%\" % (float(errorCount)/m))\n",
    "    dataArr,labelArr = loadImages('testDigits')\n",
    "    errorCount = 0\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()\n",
    "    m,n = np.shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1   \n",
    "    print(\"测试集错误率: %.2f%%\" % (float(errorCount)/m))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    testDigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO算法实现部分跟上文是一样的，我们新创建了img2vector()、loadImages()、testDigits()函数，它们分别用于二进制图形转换、图片加载、训练SVM分类器。我们自己的SVM分类器是个二类分类器，所以在设置标签的时候，将9作为负类，其余的0-8作为正类，进行训练。这是一种'ovr'思想，即one vs rest，就是对一个类别和剩余所有的类别进行分类。如果想实现10个数字的识别，一个简单的方法是，训练出10个分类器。这里简单起见，只训练了一个用于分类9和其余所有数字的分类器。\n",
    "\n",
    "可以看到，虽然我们进行了所谓的\"优化\"，但是训练仍然很耗时，迭代10次，花费了307.4s。因为我们没有多进程、没有设置自动的终止条件，总之一句话，需要优化的地方太多了。尽管如此，我们训练后得到的结果还是不错的，可以看到训练集错误率为0，测试集错误率也仅为0.01%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，就是讲解本文的重头戏：sklearn.svm.SVC。\n",
    "## sklearn.svm.SVC\n",
    "官方英文文档手册：https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "sklearn.svm模块提供了很多模型供我们使用，本文使用的是svm.SVC，它是基于libsvm实现的。![jupyter](./img/svm7-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明如下：\n",
    "\n",
    "C：惩罚项，float类型，可选参数，默认为1.0，C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力降低，也就是对测试数据的分类准确率降低。相反，减小C的话，容许训练样本中有一些误分类错误样本，泛化能力强。对于训练样本带有噪声的情况，一般采用后者，把训练样本集中错误分类的样本作为噪声。\n",
    "\n",
    "kernel：核函数类型，str类型，默认为'rbf'。可选参数为：\n",
    "    'linear'：线性核函数\n",
    "    'poly'：多项式核函数\n",
    "    'rbf'：径像核函数/高斯核\n",
    "    'sigmod'：sigmod核函数\n",
    "    'precomputed'：核矩阵\n",
    "    \n",
    "precomputed表示自己提前计算好核函数矩阵，这时候算法内部就不再用核函数去计算核矩阵，而是直接用你给的核矩阵，核矩阵需要为n*n的。\n",
    "\n",
    "degree：多项式核函数的阶数，int类型，可选参数，默认为3。这个参数只对多项式核函数有用，是指多项式核函数的阶数n，如果给的核函数参数是其他核函数，则会自动忽略该参数。\n",
    "\n",
    "gamma：核函数系数，float类型，可选参数，默认为auto。只对'rbf' ,'poly' ,'sigmod'有效。如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features。\n",
    "\n",
    "coef0：核函数中的独立项，float类型，可选参数，默认为0.0。只有对'poly' 和,'sigmod'核函数有用，是指其中的参数c。\n",
    "\n",
    "probability：是否启用概率估计，bool类型，可选参数，默认为False，这必须在调用fit()之前启用，并且会fit()方法速度变慢。\n",
    "\n",
    "shrinking：是否采用启发式收缩方式，bool类型，可选参数，默认为True。\n",
    "\n",
    "tol：svm停止训练的误差精度，float类型，可选参数，默认为1e^-3。\n",
    "\n",
    "cache_size：内存大小，float类型，可选参数，默认为200。指定训练所需要的内存，以MB为单位，默认为200MB。\n",
    "\n",
    "class_weight：类别权重，dict类型或str类型，可选参数，默认为None。给每个类别分别设置不同的惩罚参数C，如果没有给，则会给所有类别都给C=1，即前面参数指出的参数C。如果给定参数'balance'，则使用y的值自动调整与输入数据中的类频率成反比的权重。\n",
    "\n",
    "verbose：是否启用详细输出，bool类型，默认为False，此设置利用libsvm中的每个进程运行时设置，如果启用，可能无法在多线程上下文中正常工作。一般情况都设为False，不用管它。\n",
    "\n",
    "max_iter：最大迭代次数，int类型，默认为-1，表示不限制。\n",
    "\n",
    "decision_function_shape：决策函数类型，可选参数'ovo'和'ovr'，默认为'ovr'。'ovo'表示one vs one，'ovr'表示one vs rest。\n",
    "\n",
    "random_state：数据洗牌时的种子值，int类型，可选参数，默认为None。伪随机数发生器的种子,在混洗数据时用于概率估计。\n",
    "\n",
    "其实，只要自己写了SMO算法，每个参数的意思，大概都是能明白的。\n",
    "## 编写代码\n",
    "SVC很是强大，我们不用理解算法实现的具体细节，不用理解算法的优化方法。同时，它也满足我们的多分类需求。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import operator\n",
    "from os import listdir\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def img2vector(filename):\n",
    "    \"\"\"\n",
    "    将32x32的二进制图像转换为1x1024向量。\n",
    "    Parameters:\n",
    "        filename - 文件名\n",
    "    Returns:\n",
    "        returnVect - 返回的二进制图像的1x1024向量\n",
    "    \"\"\"\n",
    "    #创建1x1024零向量\n",
    "    returnVect = np.zeros((1, 1024))\n",
    "    #打开文件\n",
    "    fr = open(filename)\n",
    "    #按行读取\n",
    "    for i in range(32):\n",
    "        #读一行数据\n",
    "        lineStr = fr.readline()\n",
    "        #每一行的前32个元素依次添加到returnVect中\n",
    "        for j in range(32):\n",
    "            returnVect[0, 32*i+j] = int(lineStr[j])\n",
    "    #返回转换后的1x1024向量\n",
    "    return returnVect\n",
    "\n",
    "def handwritingClassTest():\n",
    "    \"\"\"\n",
    "    手写数字分类测试\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    #测试集的Labels\n",
    "    hwLabels = []\n",
    "    #返回trainingDigits目录下的文件名\n",
    "    trainingFileList = listdir('trainingDigits')\n",
    "    #返回文件夹下文件的个数\n",
    "    m = len(trainingFileList)\n",
    "    #初始化训练的Mat矩阵,测试集\n",
    "    trainingMat = np.zeros((m, 1024))\n",
    "    #从文件名中解析出训练集的类别\n",
    "    for i in range(m):\n",
    "        #获得文件的名字\n",
    "        fileNameStr = trainingFileList[i]\n",
    "        #获得分类的数字\n",
    "        classNumber = int(fileNameStr.split('_')[0])\n",
    "        #将获得的类别添加到hwLabels中\n",
    "        hwLabels.append(classNumber)\n",
    "        #将每一个文件的1x1024数据存储到trainingMat矩阵中\n",
    "        trainingMat[i,:] = img2vector('trainingDigits/%s' % (fileNameStr))\n",
    "    clf = SVC(C=200,kernel='rbf')\n",
    "    clf.fit(trainingMat,hwLabels)\n",
    "    #返回testDigits目录下的文件列表\n",
    "    testFileList = listdir('testDigits')\n",
    "    #错误检测计数\n",
    "    errorCount = 0.0\n",
    "    #测试数据的数量\n",
    "    mTest = len(testFileList)\n",
    "    #从文件中解析出测试集的类别并进行分类测试\n",
    "    for i in range(mTest):\n",
    "        #获得文件的名字\n",
    "        fileNameStr = testFileList[i]\n",
    "        #获得分类的数字\n",
    "        classNumber = int(fileNameStr.split('_')[0])\n",
    "        #获得测试集的1x1024向量,用于训练\n",
    "        vectorUnderTest = img2vector('testDigits/%s' % (fileNameStr))\n",
    "        #获得预测结果\n",
    "        # classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n",
    "        classifierResult = clf.predict(vectorUnderTest)\n",
    "        print(\"分类返回结果为%d\\t真实结果为%d\" % (classifierResult, classNumber))\n",
    "        if(classifierResult != classNumber):\n",
    "            errorCount += 1.0\n",
    "    print(\"总共错了%d个数据\\n错误率为%f%%\" % (errorCount, errorCount/mTest * 100))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    handwritingClassTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，训练和测试的时间总共加起来才7.3s。而且，测试集的错误率仅为1.37%。试着改变SVC的参数，慢慢体会一下吧~\n",
    "\n",
    "# 总结\n",
    "## SVM的优缺点\n",
    "优点\n",
    "\n",
    "可用于线性/非线性分类，也可以用于回归，泛化错误率低，也就是说具有良好的学习能力，且学到的结果具有很好的推广性。\n",
    "可以解决小样本情况下的机器学习问题，可以解决高维问题，可以避免神经网络结构选择和局部极小点问题。\n",
    "SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。\n",
    "\n",
    "缺点\n",
    "对参数调节和和函数的选择敏感。\n",
    "## 其他\n",
    "至此，关于SVM的文章已经写完，还有一些理论和细节，可能会在今后的文章提及。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
