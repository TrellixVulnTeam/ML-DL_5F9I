{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "    机器学习及应用,王荣贵,机械工业出版社\n",
    "    机器学习，周志华，清华大学出版社\n",
    "\n",
    "机器学习模型本质上都可以看成是某种映射函数，作为初始模型的映射函数一般都包含若干未知参数，需要通过对样本的学习来确定参数的合理取值。因此，机器学习的过程就是解决这些参数的取值问题。\n",
    "\n",
    "通常使用对目标函数进行优化计算的方式获得参数取值。如果初始模型比较简单，则模型求解的目标函数也比较简单。若目标函数为线性函数，可用单纯形法等常用线性规划方法获得精确解，然而，机器学习的模型主要为非线性函数或约束条件中含有的非线性函数，属于非线性规划问题。目前还没有针对此类优化计算问题的精确解法，而是使用具有针对线性的近似计算方法进行模型参数求解，构造具有一定精度的近似优化模型。\n",
    "\n",
    "对于比较简单的非线性目标函数，通常使用**参数估计方式**直接对模型参数进行近似估计，对于较为复杂的非线性目标函数，直接对其进行参数估计一般难以取得满意的效果，此时通常使用迭代计算或动态规划方式逐步优化模型参数估计值。此外，还需要采用一些特定策略对模型进行正则化处理以消除模型中存在的过拟合现象。\n",
    "\n",
    "本章主要介绍模型优化近似计算方法，包括模型参数的基本估计方法，集中模型优化近似计算方法买入基本的近似优化方法和概率近似优化方法，及正则化的概念和常用策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数估计\n",
    "对于特定的机器学习任务，确定机器学习模型结构之后，给出不同的参数时，模型的性能一般会存在差异。如何选择一组参数使得模型对具体任务的表现达到最优，是参数估计要解决的主要问题。\n",
    "\n",
    "在统计学中，根据从总体中抽取的随机样本来估计模型未知参数的过程被称为**参数估计**（parameter estimation）。 常用的参数估计方法有：**最小二乘估计，最大似然估计和最大后验估计**，其中最小二乘估计用于函数模型的参数估计，最大似然估计和最大后验估计常用于概率模型的参数估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘估计\n",
    "最小二乘估计是一种基于误差平方和最小化的参数估计方法。对于线性模型，最小二乘估计量是一种具有最小方差的无偏估计量，由其求得的参数估计值是最优估计值。最小二乘估计计算简单、易于理解且具有良好的实际意义。因此，最小二乘法是对线性统计模型进行参数估计的基本方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 算法原理\n",
    "最小二乘估计（Ordinary Least Square，OLS），又称最小二乘法，它的核心思想是：最小化实际值与预测值之间的误差平方和，使得误差平方和最小的参数就是最优估计值。 以**线性**回归问题为例，其误差函数可以表示为：\n",
    "$$\\begin{eqnarray*}\n",
    "\\ell&=&\\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)}-w^Tx^{(i)})^2\\\\\n",
    "&=&\\frac{1}{2m}\\lVert y-wX\\rVert_2^2\\\\\n",
    "&=&\\frac{1}{2m}(y-wX)(y-wX)^T\n",
    "\\end{eqnarray*}$$\n",
    "当目标函数取最小值时，所对应的模型参数为最优。设此时参数为$\\hat w$，$$\\hat w=arg_w min \\ell$$函数极值点处对所有参数的偏导为0，故可由此求出最小二乘估计值。\n",
    "$$\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell}{\\partial w}&=&\\frac{\\partial}{\\partial w}\\frac{1}{2m}(yy^T-2wXy^T+wXX^Tw^T)\\\\\n",
    "&=&\\frac{1}{2m}(-2Xy^T+2XX^Tw^T)=0\\\\\n",
    "&\\Leftrightarrow &-Xy^T+XX^Tw^T=0\\\\\n",
    "&\\Leftrightarrow &XX^Tw^T=Xy^T\\\\\n",
    "&\\Leftrightarrow &w=(X^TX)^{-1}X^Ty\n",
    "\\end{eqnarray*}$$\n",
    "所求结果即是$\\hat w$。\n",
    "用平方和的原因是因为回归函数的预测值与真实值之差可正可负，简单求和可能出现误差抵消的情况，而采用平方误差既保证了非负性，也没有破坏误差关系。因此，最小二乘估计是回归问题中最常用的参数估计方法。\n",
    "\n",
    "可见，最小二乘法（又称最小平方法）是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：有如下示例，产值Q和劳动力投入L之间满足$Q=aL^b$关系，其中$a,b$为未知参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>年份</th>\n",
       "      <th>劳动力投入</th>\n",
       "      <th>产值</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>42</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>51</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>49</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>65</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>57</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     年份  劳动力投入   产值\n",
       "0  2013     42  188\n",
       "1  2014     51  210\n",
       "2  2015     49  194\n",
       "3  2016     65  207\n",
       "4  2017     57  221"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#在下面添加此代码以显示单元格中的所有输出\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "product={\n",
    "    '年份':['2013','2014','2015','2016','2017'],\n",
    "    '劳动力投入':[42,51,49,65,57],\n",
    "    '产值':[188,210,194,207,221]\n",
    "}\n",
    "#定义数据库\n",
    "proDf=pd.DataFrame(product)\n",
    "proDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q=aL^b$不是线性关系，通过对数运算将其转化为线性关系;\n",
    "$$lnQ=lna+blnL$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令：\n",
    "$y_i=lnQ,x_i=lnL,\\beta _0=lna,\\beta_1=b$，将样本改为形式\n",
    "$$X_i=\n",
    "\\begin{eqnarray*}\\left [\\matrix{\n",
    "1\\\\\n",
    "x_i}\n",
    "\\right ] \\end{eqnarray*}$$则有$$f(X)=\\beta_0+\\beta_1x_i=w^TX$$这里$w=[\\beta_0,\\beta_1]^T$，是需要求的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据最小二乘法，采用最小方差构造优化目标（$\\ell$中的系数不影响结果，故省略）：$$\\sum_0^4(f(X_i)-y_i)^2=\\sum_0^4(\\beta_0+\\beta_1x_i-y_i)^2$$\n",
    "分别对$\\beta_0,\\beta_1$求导并令导数值为0：\n",
    "$$\\frac{\\partial \\ell}{\\partial \\beta_0}=2\\sum_0^4(\\beta_0+\\beta_1x_i-y_i)(-1)=0$$\n",
    "$$\\frac{\\partial \\ell}{\\partial \\beta_1}=2\\sum_0^4(\\beta_0+\\beta_1x_i-y_i)(-x_i)=0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可转化为**一般形式**:\n",
    "$$\\beta_0N+\\beta_1\\sum x_i=\\sum y_i$$\n",
    "$$\\beta_0\\sum x_i+\\beta_1\\sum x_i^2=\\sum x_iy_i$$其中N为样本数量。带入样本数据，可求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python求参数代码\n",
    "根据原理，求参数公式为$w=(X^TX)^{-1}X^Ty$，下面用python实现函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在下面添加此代码以显示单元格中的所有输出\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 3.73766962],\n",
       "       [1.        , 3.93182563],\n",
       "       [1.        , 3.8918203 ],\n",
       "       [1.        , 4.17438727],\n",
       "       [1.        , 4.04305127]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([5.23644196, 5.34710753, 5.26785816, 5.33271879, 5.3981627 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[4.19517866],\n",
       "        [0.28345546]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standRegres(xArr,yArr):\n",
    "    \"\"\"\n",
    "    函数说明:计算回归系数w\n",
    "    Parameters:\n",
    "        xArr - x数据集(第一列为1)\n",
    "        yArr - y数据集\n",
    "    Returns:\n",
    "        ws - 回归系数\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T    \n",
    "    xTx = xMat.T * xMat                            #根据文中推导的公示计算回归系数\n",
    "    #print(xMat.shape,xArr)\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"矩阵为奇异矩阵,不能求逆\")\n",
    "        return\n",
    "    ws = xTx.I * xMat.T*yMat #w  公式\n",
    "    return ws\n",
    "\n",
    "x_=np.ones([proDf.shape[0],2])\n",
    "x_[:,1]=np.array(np.log(proDf.iloc[:,1]))#提取两列数据，分别作为x,y\n",
    "x_\n",
    "y_=outValue=np.array(np.log(proDf.iloc[:,2]))\n",
    "y_\n",
    "ws=standRegres(x_,y_)\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果为$\\beta_0=4.19517866,\\beta_1=0.28345546$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[66.36558706]]), matrix([[0.28345546]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(ws[0]),ws[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此得到产值Q和劳动力投入L之间满足$Q=66.36L^{0.283}$关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python拟合直线实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据上面的一般形式，可求解。首先引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别取proDf二维数组中的三列：年份、劳动力、产值等作为导入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x256e472d348>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#year=proDf.iloc[:,0]\n",
    "x=labor=np.log(proDf.iloc[:,1])#提取两列数据，分别作为x,y\n",
    "y=outValue=np.log(proDf.iloc[:,2])\n",
    "#用plt画出散点图\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy 的 linalg 模块中有一个 $solve$函数，它可以根据方程组的系数矩阵和方程右端构成的向量来求解未知量$\\beta_0$ 与 $\\beta_1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.195178654999998, 0.28345546174325226)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_regression(x, y): \n",
    "     N = len(x)\n",
    "     sumx = sum(x)\n",
    "     sumy = sum(y)\n",
    "     sumx2 = sum(x**2)\n",
    "     sumxy = sum(x*y)\n",
    " \n",
    "     A = np.mat([[N, sumx], [sumx, sumx2]])\n",
    "     b = np.array([sumy, sumxy])\n",
    " \n",
    "     return np.linalg.solve(A, b)\n",
    " \n",
    "b0,b1 = linear_regression(x,y)\n",
    "b0,b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用最小二乘法的正则方程组来求解未知系数 $\\beta_0$ 与 $\\beta_1$,和上面的结果一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出拟合曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x256e47dfcc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x256e465b108>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "pred_y= b1*x+b0\n",
    "plt.plot(x,pred_y,c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算损失$\\ell$，如上知道公式$\\ell=\\frac{1}{2m}(y-wX)(y-wX)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0079952482363221\n",
      "0.0079952482363221\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "#w=mat(array([b0,b1]))\n",
    "wx=mat(ones([2,len(x)]))\n",
    "wx[1,:]=array(x).T\n",
    "pred_yi=array(y.T)-array((dot([b0,b1],wx)))#矩阵乘法\n",
    "print(sum(pred_yi**2))\n",
    "#也可以直接用上式的pred_y\n",
    "print(sum((y-pred_y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python拟合曲线实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "签数是以线性回归为例子，如果是拟合曲线，需要将上述一般式展开，成为下面的形式![jupyter](./img/model-1.png)\n",
    "如曲线的方程为 $y = a_0 + a_1*x + a_2*x^2$，根据公式求解$ a_0、a_1 和 a_2$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x256e485bb48>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#生成服从正态分布的样本点\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# y = 2 + 3x + 4x^2\n",
    "X = np.arange(0, 5, 0.1) \n",
    "Z = [2 + 3 * x + 4 * x ** 2 for x in X] \n",
    "Y = np.array([np.random.normal(z,3) for z in Z])\n",
    " \n",
    "plt.plot(X, Y, 'ro') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成系数矩阵A\n",
    "def gen_coefficient_matrix(X, Y): \n",
    " N = len(X)\n",
    " m = 3\n",
    " A = []\n",
    " # 计算每一个方程的系数\n",
    " for i in range(m):\n",
    "  a = []\n",
    "  # 计算当前方程中的每一个系数\n",
    "  for j in range(m):\n",
    "   a.append(sum(X ** (i+j)))\n",
    "  A.append(a)\n",
    " return A\n",
    " \n",
    "# 计算方程组的右端向量b\n",
    "def gen_right_vector(X, Y): \n",
    " N = len(X)\n",
    " m = 3\n",
    " b = []\n",
    " for i in range(m):\n",
    "  b.append(sum(X**i * Y))\n",
    " return b\n",
    " \n",
    "A = gen_coefficient_matrix(X, Y) \n",
    "b = gen_right_vector(X, Y)\n",
    " \n",
    "a0, a1, a2 = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据求得的曲线方程，绘制出曲线的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x256e48cdd08>,\n",
       " <matplotlib.lines.Line2D at 0x256e486a408>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'y = 0.9374280078418962 + 2.857170913968501x + 4.06470591621364$x^2$ ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 生成拟合曲线的绘制点\n",
    "_X = np.arange(0, 5, 0.1) \n",
    "_Y = np.array([a0 + a1*x + a2*x**2 for x in _X])\n",
    " \n",
    "plt.plot(X, Y, 'ro', _X, _Y, 'b', linewidth=2) \n",
    "plt.title(\"y = {} + {}x + {}$x^2$ \".format(a0, a1, a2)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在概率模型中，最大似然估计（Maximum Likelihood Estimation，MLE）是最常用的参数估计方法。首先需要明白什么是似然，根据贝叶斯公式：\n",
    "$$p(\\theta∣X)=\\frac{p(\\theta,X)}{p(X)}=\\frac{p(X∣\\theta )p(\\theta)}{p(X)}$$\n",
    "其中，$p(X)$描述的是样本X的发生概率，对于给定样本X,，其值与参数$\\theta$无关，因此，可以得到如下公式：$$p(\\theta∣X)∝p(X∣\\theta)p(\\theta)$$\n",
    "上式中，$p(\\theta)$是参数先验（prior）概率，即在没有看到样本情况下，我们对模型的认知。\n",
    "\n",
    "$p(X∣\\theta)$是在给定参数$\\theta$的情况下，样本产生的概率，称为**似然**（likelihood）。\n",
    "\n",
    "$p(\\theta∣X)$是参数的**后验（posterior）概率**，即在看到样本分布后，我们对模型认知的调整。\n",
    "\n",
    "回到最大似然估计，顾名思义，就是要最大化似然函数，使得**似然函数最大的那组参数就是我们要找的最优参数**。 我们知道，对于样本空间，给定模型后，不同参数会对样本空间有不同的描述。现在我们只有从样本空间独立抽取的若干样本\n",
    "$$X=\\{x^{(1)},x^{(2)},\\dots,x^{(m)}\\}$$\n",
    "需要判断哪组参数是对整个样本空间描述最好的那组参数，很自然的想法是最大化样本效益，即找到使得样本X产生概率最大的那组参数。因为样本X是已经观测到的，相当于我们的证据，选择对证据描述最好的那组参数肯定是最保险最稳妥的做法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要叫似然呢？我们要区分概率与似然：\n",
    "\n",
    "    概率： 在给定参数的情况下，事件发生的可能性。\n",
    "    似然： 在已知某些观测结果的情况下，参数取值的可能性。\n",
    "可以看到概率和似然是两个完全不同的角度，概率是由因求果，似然是由果执因。概率要求满足概率之和等于1，但似然不要求满足归一性。两者的联系在于，似然的取值等于在给定参数的情况下，事件发生的概率。所以**似然函数是关于参数的函数**，其数学形式如下：\n",
    "$$\\ell(\\theta | X)=p(X;\\theta)$$\n",
    "似然函数乘以一个正常数后仍然是似然函数，其取值并不需要满足归一性：$$\\sum_\\theta a\\ell(\\theta | X)\\neq 1,a>1$$\n",
    "对$\\theta$的最大似然估计是：$$\\theta_{ML}=argmax_\\theta p(X;\\theta)=argmax_\\theta \\prod_{i=1}^{M}p(x^{(i)};\\theta)$$\n",
    "多个概率的乘积可能出现数值下溢，常取对数将乘积转化为求和，得到最大对数似然：$$\\theta_{ML}=argmax_\\theta \\prod_{i=1}^{M}log p(x^{(i)};\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：假设一个盒子里有三颗围棋子，采用放回抽样随机抽取三次，得到白子2次，黑子1次。试用最大似然估计法估计盒子中白子个数。\n",
    "\n",
    "设盒子中有$\\theta$($\\theta=0,1,2,3$)颗白子，$p(白|\\theta)$为一次采样的概率，分别为0、1/3、2/3、3/3。由于三次采用中抽到了两次白子，则似然函数为$$\\ell(\\theta)=[p(白|\\theta)]^2[1-p(白|\\theta)]$$\n",
    "$\\theta=0,1,2,3$时，似然函数值分别为0、2/27、4/27、0。取最大的4/27，即$\\theta=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本X为连续随机变量时，可**用其概率密度函数$f(X;\\theta)$来构造似然函数**。即有\n",
    "$$\\ell(\\theta|X)=\\prod_{i=1}^{M}f(X_i;\\theta)$$对似然函数进行最大优化计算，即可得到对参数$\\theta$的估计值。这里，多个连续函数相乘，难以求解，同样采用对数似然函数的形式。\n",
    "\n",
    "例子：下表为某学校的学生身高抽样，已知身高服从正态分布$N(u,\\sigma^2)$。试根据抽样数据计算正态分布的参数$u,\\sigma$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>身高(厘米)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   编号  身高(厘米)\n",
       "0   0     171\n",
       "1   1     164\n",
       "2   2     174\n",
       "3   3     165\n",
       "4   4     168\n",
       "5   5     181\n",
       "6   6     176\n",
       "7   7     162\n",
       "8   8     173\n",
       "9   9     172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "student={\n",
    "    '编号':[0,1,2,3,4,5,6,7,8,9],\n",
    "    '身高(厘米)':[171,164,174,165,168,181,176,162,173,172]\n",
    "}\n",
    "#定义数据库\n",
    "studentDf=pd.DataFrame(student)\n",
    "studentDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正态分布的概率密度为$$f(X_i;u,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left [-\\frac{(X-u)^2}{2\\sigma^2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造似然函数：$$\\ell(\\theta|X)=\\prod_{i=0}^{9}f(X_i;\\theta)=\\prod_{i=0}^{9}f(X_i;u,\\sigma^2)=\\prod_{i=0}^{9}\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left [-\\frac{(X-u)^2}{2\\sigma^2}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数为：\n",
    "$$ln\\ell(\\theta|X)=ln\\ell(u,\\sigma^2|X)=-5ln2\\pi-10ln\\sigma-\\frac{\\sum_{i=0}^9(X-u)^2}{2\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$ln\\ell(\\theta|X)$分别求$u,\\sigma^2$求偏导并令其为0，可得：\n",
    "$$\\frac{\\partial ln\\ell(u,\\sigma^2|X)}{\\partial u}=\n",
    "\\frac{\\sum_{i=0}^9(X_i-u)}{\\sigma^2}=0\n",
    "$$\n",
    "$$\\frac{\\partial ln\\ell(u,\\sigma^2|X)}{\\partial \\sigma^2}=-\\frac{5}{\\sigma^2}+\n",
    "\\frac{\\sum_{i=0}^9(X_i-u)^2}{2\\sigma^4}=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解得：\n",
    "$$\\hat u=\\bar X=\\frac{1}{10}\\sum_{i=0}^9X_i$$\n",
    "$$\\hat {\\sigma^2}=\\frac{1}{10}\\sum_{i=0}^9(X_i-\\bar X)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170.6, 31.240000000000002)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_=np.array(studentDf.iloc[:,1])#提取两列数据，分别作为x,y\n",
    "u=sum(x_)/10\n",
    "sigma=sum((x_-u)**2)/10\n",
    "u,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘估计与最大似然估计的联系\n",
    "最小二乘估计用于函数模型的参数估计，最大似然估计常用于概率模型的参数估计，那最小二乘估计与最大似然估计有没有什么联系呢？对于回归问题，模型的输出直接是一个单独的实数值，其实我们也可以将其转化为一个概率问题，即在给定输入$x$的情况下，$y$取到取值空间中每个值得概率$p(y∣x)$，概率最大对应的$y$就是我们的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大后验估计\n",
    "最大似然估计属于频率学派的观点，即参数未知，但取值固定，但贝叶斯学派认为参数不仅未知而且取值不固定，最大后验估计认为待求参数服从某一未知概率分布。**参数以一定的概率取某一特定值。**\n",
    "\n",
    "在进行参数估计时，最大后验估计依据过往经验和已经出现的样本共同确定参数的可能取值。以抛硬币为例，现在期望估计硬币正面向上的概率$\\theta$，依据以往经验，硬币正面向上的概率$\\theta$一般是0.5，但是考虑到硬币个体可能存在某些特点，故没有将硬币概率确定为0.5，而是给出关于$\\theta$取值的一个概率分布函数$g(\\theta)$,如:\n",
    "$$g(\\theta)=\n",
    "\\begin{eqnarray*}\\left \\{\\matrix{\n",
    "0.9,\\theta=0.5\\\\\n",
    "0.1,\\theta\\neq0.5}\n",
    "\\right.  \\end{eqnarray*}$$\n",
    "$g(\\theta)$被称为参数$\\theta$的**先验概率分布或先验概率**，表示根据过往经验得到$\\theta$取值的概率。假如抛掷10次硬币，7次正面，3次反面，则最大后验估计希望根据样本情况对参数值进行估计。即考虑在样本值已经出现的情况下计算$\\theta$取值**条件概率**$f(\\theta|X)$。\n",
    "\n",
    "其中，$X$表示已经出现的样本取值情况，$f(\\theta|X)$被称为后验概率。可以看成是根据样本数据出现的情况对先验概率$g(\\theta)$的修正。**后验概率最大时所对应的参数取值即为所求的最大后验估计值。**既有$$\\hat\\theta=argmax_{\\theta}f(\\theta|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由贝叶斯公式，有：$$f(\\theta|X)=\\frac{f(X|\\theta)g(\\theta)}{p(X)}$$\n",
    "$f(X|\\theta)$为现有样本表现出来的信息，分母$p(X)$为样本的分布，$p(X)$与样本无关且恒大于0，故**可以直接用分子$f(X|\\theta)g(\\theta)$最大化的优化方式实现最大后验估计。**$$\\hat \\theta=argmax_{\\theta}f(X|\\theta)g(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本小节硬币例子中，$\\theta=0.5$的情况下，“7次正面，3次反面”的条件概率为：$$f(X=7,3|\\theta=0.5)=C_{10}^7\\theta^7(1-\\theta)^3=0.1171875$$\n",
    "可得：$$f(X=7,3|\\theta=0.5)g(\\theta)=0.10546875$$由于是条件概率，故$$f(X=7,3|\\theta\\neq0.5)g(\\theta)<0.1<f(X=7,3|\\theta=0.5)g(\\theta)$$\n",
    "由此可知，最大后验估计$\\hat \\theta=0.5$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过分析可知，尽管已知样本的取值状况与过往经验不符合，但是由于过往经验比较可靠，故最大后验估计在结论上选择相信了经验而非实际样本所表现出来的信息，同时也说明样本这样的信息表现是由于随机波动造成的。如果使用最大似然估计，很可能得到的结果是$\\hat \\theta=0.7$。但是由于试验次数较少，结果可能存在较大的波动，这种情况下，如果只考虑最大似然估计，可能会与参数的真实值有较大的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是**多参数情况**，可将最大后验估计表示为：$$\\hat \\beta=argmax_{\\beta}f(X|\\beta)g(\\beta)$$也可以用等价的对数形式表示。其中$\\beta=(\\beta_1,\\dots,\\beta_k)$为未知参数向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：假设某公司过去三年员工收入服从均值为6（万元）、方差为0.36（万元）的正态分布。下表是随机抽取的10名员工的收入数据。根据表中数据和过去三年员工搜如情况，估计今年员工的均值和方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>收入</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   编号   收入\n",
       "0   1  6.1\n",
       "1   2  5.3\n",
       "2   3  7.1\n",
       "3   4  7.3\n",
       "4   5  6.4\n",
       "5   6  5.9\n",
       "6   7  6.7\n",
       "7   8  6.3\n",
       "8   9  5.6\n",
       "9  10  6.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "income={\n",
    "    '编号':[1,2,3,4,5,6,7,8,9,10],\n",
    "    '收入':[6.1,5.3,7.1,7.3,6.4,5.9,6.7,6.3,5.6,6.5]\n",
    "}\n",
    "#定义数据库\n",
    "incomeDf=pd.DataFrame(income)\n",
    "incomeDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知正态分布的概率密度为$$f(X;u,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left [-\\frac{(X-u)^2}{2\\sigma^2}\\right]$$\n",
    "则收入X的先验概率为$$f(X_i;6,0.36)=\\frac{1}{\\sqrt{2\\pi}0.6}exp\\left [-\\frac{(X-6)^2}{0.72}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后验概率为$$f(u,\\sigma^2|X_k)=f(X;6.0.36)\\prod_{k=1}{10}f(X_k|u.\\sigma^2)$$为求最大后验概率，分别求$u,\\sigma^2$的偏导并令其为0,类似于最大似然估计，可求得$\\hat u,\\hat \\sigma^2$，最后结果为$\\hat u=6.4,\\hat \\sigma^2=0.72$(结果可能不太准确)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型优化基本方法\n",
    "**模型复杂时，很难通过参数估计方法得到最优估计值**。事实上，机器学习的模型训练除了使用参数估计法之外，还可以通过数字优化方法确定模型参数，这类数字优化方法通常采用迭代逼近的方式确定最优解，此时模型性能会得到提升，故称此方法为模型优化方法。在很多情况下，模型优化方法能够有效应对优化目标较为复杂的情况。\n",
    "\n",
    "模型优化方法很多，这里主要介绍梯度下降法和牛顿迭代法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "参考：https://blog.csdn.net/pxhdky/article/details/82430196\n",
    "\n",
    "梯度下降法（Gradient Descent）又称最速下降法（Steepest descent）是一种常用的一阶优化方法，是一种用于求解无约束最优化问题的最常用的方法。它选取适当的初始值，并不断向负梯度方向迭代更新，实现目标函数的极小化，直到收敛。\n",
    "\n",
    "以下山法作为例子，我们想要从山的某个位置下山，但我们并不知道山脚的位置，只能走一步算一步。从当前位置出发，往当前位置的负梯度方向走一步，即往最陡峭的方向往下走一步。然后继续求解当前位置的梯度，往负梯度方向走一步。不停走下去，一直走到我们认为已经到了山脚的位置。当然，也有可能，我们没办法到山脚，而是到了一个小山丘底部。\n",
    "\n",
    "当目标函数是凸函数的时候，梯度下降法可以确保找到全局最优解；否则不一定能找到全局最优解，可能会陷入局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法的原理\n",
    "考虑最优化问题$min_x f(x)$，其中$f(x)$具有一阶连续偏导数。若第$k$次迭代值为$x^{(k)}$，对$f(x)$在$x^{(k)}$处进行**一阶泰勒展开**：$$f(x)=f(x^{(k)})+(x-x^{(k)})\\nabla f(x^{(k)})\\tag 1$$![jupyter](./img/model-2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图，黑色为凸函数曲线，可以利用线性逼近的思想求出$f(\\theta)$。当前点为$\\theta_0$，红色为该处的斜率线，根据直线方程，很容易近似得到$$f(\\theta)=f(\\theta_0)+(\\theta-\\theta_0)\\nabla f(\\theta_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x-x^{(k)}$是微小矢量，矢量的大小是**步长**$a$，是正值标量，$x-x^{(k)}$的单位向量用$v$表示，则：$$x-x^{(k)}=av$$$$f(x)=f(x^{(k)})+av\\nabla f(x^{(k)})$$我们希望每次迭代，都能使$f(x)$变小，也就是说希望有：\n",
    "$$f(x)-f(x^{(k)})=av\\nabla f(x^{(k)})<0\\tag 2$$忽略$a$，由于$v,\\nabla f(x^{(k)})$都是向量，根据向量乘积公式将公式（2）转换：\n",
    "$$v\\nabla f(x^{(k)})=\\lVert v\\rVert\\cdot\\lVert f(x^{(k)})\\rVert cos(v,f(x^{(k)}))<0$$\n",
    "当$v$和$\\nabla f(x^{(k)})$反向时，$cos(v,f(x^{(k)}))=-1$，可以使得$av\\nabla f(x^{(k)})$最小，且为负。即$v$的方向是使局部的目标函数下降最快的方向。$v$为单位向量，得到：$$v=-\\frac{\\nabla f(x^{(k)})}{\\lVert \\nabla f(x^{(k)})\\rVert}$$\n",
    "\n",
    "以上解释了为什么局部下降最快的方向就是梯度的负方向，反之，梯度方向就是使得函数值上升最快的方向。\n",
    "\n",
    "由于$\\lVert f(x^{(k)})\\rVert$是标量，可以吸收入$a$里面，梯度下降算法的更新表达式就变成了：\n",
    "      $$x-x^{(k)}=av=-a\\nabla f(x^{(k)})$$  \n",
    "如果$\\lVert f(x^{(k)})\\rVert$不吸收入$a$里面，则$a$，可以不断更新，形成动态的步长：$$a<--\\frac{a}{\\lVert f(x^{(k)})\\rVert} \\tag 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法描述\n",
    "输入：目标函数$f(x)$、梯度函数$\\nabla f(x)$、计算精度$\\epsilon$\n",
    "\n",
    "输出：$f(x)$的极小点$x^*$\n",
    "\n",
    "（1）初始化相关参数。取初值$x^{(0)}\\in R^n$,置迭代次数$k=0$。\n",
    "\n",
    "（2）计算当前位置的目标函数$f(x^{(0)})$。\n",
    "\n",
    "（3）计算当前位置目标的梯度$\\nabla f(x^{(k)})$。如果$\\lVert \\nabla f(x^{(k)})\\rVert <\\epsilon$，迭代结束，$x^*=x^{(k)}$。否则，继续往下走。\n",
    "\n",
    "（4）更新$x$。$x^{(k+1)}=x^{(k)}-a\\nabla f(x^{(k)})$，如果$\\lVert f(x^{(k+1)})-f(x^{(k)})\\rVert<\\epsilon   $或者$\\lVert x^{(k+1)}-x^{(k)}\\rVert<\\epsilon$,停止迭代,令$x^*=x^{(k+1)}$。否则迭代次数$k=k+1$，转（3）。\n",
    "    \n",
    "在机器学习中，目标函数就是代价函数，或损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降算法简单Python实现\n",
    "对于一维问题，假设目标函数为$f(x)=x^2+1$。一眼就知道它的最小值是 x=0 处，但是这里我们需要用梯度下降法的 Python 代码来实现。从代码可以看到，其实现假设当前点，不断求梯度，并更新当前点，当达到终止条件的时候，最新的当前点就是求解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "局部最小值 x = 4.775196665967835e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.775196665967835e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#一维问题的梯度下降法示例\n",
    "\n",
    "def grad_1d(x):\n",
    "    return x * 2\n",
    "\n",
    "def gradient_descent_1d(grad, cur_x=0.1, learning_rate=0.01, precision=0.0001, max_iters=10000):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    :param grad: 目标函数的梯度\n",
    "    :param cur_x: 当前 x 值，通过参数可以提供初始值\n",
    "    :param learning_rate: 学习率，也相当于设置的步长\n",
    "    :param precision: 设置收敛精度\n",
    "    :param max_iters: 最大迭代次数\n",
    "    :return: 局部最小值 x*\n",
    "    \"\"\"\n",
    "    for i in range(max_iters):\n",
    "        grad_cur = grad(cur_x)#求梯度\n",
    "        if abs(grad_cur) < precision:\n",
    "            break  # 当梯度趋近为 0 时，视为收敛\n",
    "        cur_x = cur_x - grad_cur * learning_rate\n",
    "        #print(\"第\", i, \"次迭代：x 值为 \", cur_x)\n",
    "\n",
    "    print(\"局部最小值 x =\", cur_x)\n",
    "    return cur_x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_descent_1d(grad_1d, cur_x=10, learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码中没有使用迭代次数，而是使用梯度变化量的精度作为终止条件。具体实现中，可以使用梯度的变化量（如夹角）、迭代次数作为终止条件。\n",
    "\n",
    "接下来推广到二维，目标函数设为：$f(x,y)=-e^{-(x^2+y^2)}$，该函数在 [0,0]处有最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1] 作为初始值开始迭代...\n",
      "局部最小值 x = [ 3.18808392e-07 -3.18808392e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.18808392e-07, -3.18808392e-07])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!二维问题的梯度下降法示例\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def grad_2d(x):\n",
    "    \"\"\"\n",
    "    目标函数的梯度\n",
    "    :param x: 自变量，二维向量\n",
    "    :return: 因变量，二维向量\n",
    "    \"\"\"\n",
    "    deriv0 = 2 * x[0] * math.exp(-(x[0] ** 2 + x[1] ** 2))\n",
    "    deriv1 = 2 * x[1] * math.exp(-(x[0] ** 2 + x[1] ** 2))\n",
    "    return np.array([deriv0, deriv1])\n",
    "\n",
    "def gradient_descent_2d(grad, cur_x=np.array([0.1, 0.1]), learning_rate=0.01, precision=0.0001, max_iters=10000):\n",
    "    \"\"\"\n",
    "    二维问题的梯度下降法\n",
    "    :param grad: 目标函数的梯度\n",
    "    :param cur_x: 当前 x 值，通过参数可以提供初始值\n",
    "    :param learning_rate: 学习率，也相当于设置的步长\n",
    "    :param precision: 设置收敛精度\n",
    "    :param max_iters: 最大迭代次数\n",
    "    :return: 局部最小值 x*\n",
    "    \"\"\"\n",
    "    print(f\"{cur_x} 作为初始值开始迭代...\")\n",
    "    for i in range(max_iters):\n",
    "        grad_cur = grad(cur_x)\n",
    "        if np.linalg.norm(grad_cur, ord=2) < precision:#norm则表示范数，二范数表示传统意义上的向量长度\n",
    "            break  # 当梯度趋近为 0 时，视为收敛\n",
    "        cur_x = cur_x - grad_cur * learning_rate\n",
    "        #print(\"第\", i, \"次迭代：x 值为 \", cur_x)\n",
    "\n",
    "    print(\"局部最小值 x =\", cur_x)\n",
    "    return cur_x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_descent_2d(grad_2d, cur_x=np.array([1, -1]), learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再试着以初始值 [3,−3]处开始寻找最小值，即："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 -3] 作为初始值开始迭代...\n",
      "局部最小值 x = [ 3 -3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3, -3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gradient_descent_2d(grad_2d, cur_x=np.array([3, -3]), learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法没有找到真正的极小值点！如果仔细观察目标函数的图像，以及梯度下降法的算法原理，你就很容易发现问题所在了。在 [3,−3]处的梯度就几乎为 0 了！\n",
    "\n",
    "由于“梯度过小”，梯度下降法可能无法确定前进的方向了。即使人为增加收敛条件中的精度，也会由于梯度过小，导致迭代中前进的步长距离过小，循环时间过长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.13798785e-08 -9.13798785e-08]\n"
     ]
    }
   ],
   "source": [
    "print(grad_2d(np.array([3, -3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几种梯度下降算法\n",
    "梯度下降法实现简单，原理也易于理解，但它有自身的局限性。\n",
    "\n",
    "    （1）对于梯度过小的情况，梯度下降法可能难以求解。\n",
    "    （2）梯度下降法适合求解只有一个局部最优解的目标函数，对于存在多个局部最优解的目标函数，一般情况下梯度下降法不保证得到全局最优解（由于凸函数有个性质是只存在一个局部最优解，所以也有文献的提法是：当目标函数是凸函数时，梯度下降法的解才是全局最优解）。\n",
    "    （3）由于泰勒公式的展开是近似公式，要求迭代步长要足够小，因此梯度下降法的收敛速度并非很快的。\n",
    "\n",
    "因此有了后面很多算法对它的改进：\n",
    "\n",
    "**1.批量梯度下降法Batch Gradient Descent BGD**\n",
    "\n",
    "批量梯度下降法是梯度下降法最常用的形式。每次更新参数要使用所有的样本进行计算。\n",
    "\n",
    "**2.随机梯度下降法（Stochastic Gradient Descent，SGD）**\n",
    "\n",
    "随机梯度下降法与批量梯度下降法类似。每次更新参数只使用随机的一个样本进行计算。\n",
    "\n",
    "区别：批量梯度下降法每次使用所有数据来更新参数，训练速度慢；随机梯度下降法每次只使用一个数据来更新参数，训练速度快；但迭代方向变化大，不一定每次都朝着收敛的方向，不能很快地收敛到局部最优解。\n",
    "\n",
    "**3.小批量梯度下降法（Mini-Batch Gradient Descent，MBGD）**\n",
    "\n",
    "小批量梯度下降法是批量梯度下降法和随机梯度下降法的一个折中。每次更新参数选择一小部分数据计算。\n",
    "\n",
    "下面是我们拟合的函数的形式$$h(\\theta)=\\theta_0+\\theta_1x_1+\\dots+\\theta_nx_n=\\sum_{j=0}^n\\theta_jx_j$$具体为$$h(x)=2+2x_1+3x_2+4x_3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.98727791 2.00121553 2.99996053 4.00338783]\n"
     ]
    }
   ],
   "source": [
    "#随机梯度下降法\n",
    "import numpy as np\n",
    "\n",
    "# 构造训练数据集\n",
    "x_train = np.array([[2, 0., 3], [3, 1., 3], [0, 2., 3], [4, 3., 2], [1, 4., 4]])\n",
    "\n",
    "# 构建一个权重作为数据集的真正的权重，theta1主要是用来构建y_train，然后通过模型计算\n",
    "# 拟合的theta，这样可以比较两者之间的差异，验证模型。\n",
    "theta1 = np.array([[2 ,3, 4]]).T\n",
    "\n",
    "# 构建标签数据集,y=t1*x1+t2*x2+t3*x3+b即y=向量x_train乘向量theta+b, 这里b=2\n",
    "y_train = (x_train.dot(theta1) + np.array([[2],[2],[2],[2],[2]])).ravel() \n",
    "\n",
    "# 构建一个5行1列的单位矩阵x0，然它和x_train组合，形成[x0, x1, x2, x3]，x0=1的数据形式，\n",
    "# 这样可以将y=t1*x1+t2*x2+t3*x3+b写为y=b*x0+t1*x1+t2*x2+t3*x3即y=向量x_train乘向\n",
    "# 量theta其中theta应该为[b, *, * , *]，则要拟合的theta应该是[2,2,3,4]，这个值可以\n",
    "# 和算出来的theta相比较，看模型的是否达到预期\n",
    "x0 = np.ones((5, 1))\n",
    "input_data = np.hstack([x0, x_train])#矩阵在行上合并\n",
    "m, n = input_data.shape\n",
    "\n",
    "# 设置两个终止条件\n",
    "loop_max = 100000\n",
    "epsilon = 1e-6\n",
    "\n",
    "# 初始化theta（权重）\n",
    "np.random.seed(0)\n",
    "theta = np.random.rand(n).T # 随机生成10以内的，n维1列的矩阵\n",
    "\n",
    "# 初始化步长/学习率\n",
    "alpha = 0.001\n",
    "# 初始化迭代误差（用于计算梯度两次迭代的差）\n",
    "error = np.zeros(n)\n",
    "\n",
    "# 初始化偏导数矩阵\n",
    "diff = np.zeros(n)\n",
    "\n",
    "# 初始化循环次数\n",
    "count = 0\n",
    "\n",
    "while count < loop_max:\n",
    "    count += 1  # 没运行一次count加1，以此来总共记录运行的次数\n",
    "    \n",
    "    # 计算梯度\n",
    "    for i in range(m):\n",
    "        # 计算每个维度theta的梯度，并运用一个梯度去更新它\n",
    "        diff = input_data[i].dot(theta)-y_train[i]\n",
    "        theta = theta - alpha * diff*(input_data[i])\n",
    "    \n",
    "    # else中将前一个theta赋值给error,theta - error便表示前后两个梯度的变化，当梯度\n",
    "    #变化很小（在接收的范围内）时，便停止迭代。\n",
    "    if np.linalg.norm(theta - error) < epsilon: # 判断theta与零向量的距离是否在误差内\n",
    "        break\n",
    "    else:\n",
    "        error = theta  \n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.16676836]\n",
      " [1.9841025 ]\n",
      " [3.00035775]\n",
      " [3.95560204]]\n"
     ]
    }
   ],
   "source": [
    "#批量梯度下降法import numpy as np\n",
    "\n",
    "# 构造训练数据集\n",
    "x_train = np.array([[2, 0., 3], [3, 1., 3], [0, 2., 3], [4, 3., 2], [1, 4., 4]])\n",
    "m = len(x_train)#列表长度5\n",
    "\n",
    "x0 = np.full((m, 1), 1)#可以生成一个元素为1,形状为(m,1)的array\n",
    "\n",
    "# 构造一个每个数据第一维特征都是1的矩阵\n",
    "input_data = np.hstack([x0, x_train])\n",
    "m, n = input_data.shape\n",
    "theta1 = np.array([[2 ,3, 4]]).T\n",
    "# 构建标签数据集，后面的np.random.randn是将数据加一点噪声，以便模拟数据集。\n",
    "#y_train = (input_data.dot(np.array([1, 2, 3, 4]).T)).T\n",
    "y_train = x_train.dot(theta1) + np.array([[2],[2],[2],[2],[2]]) \n",
    "\n",
    "# 设置两个终止条件\n",
    "loop_max = 10000\n",
    "epsilon = 1e-5\n",
    "\n",
    "# 初始theta\n",
    "np.random.seed(0)  # 设置随机种子\n",
    "theta = np.random.randn(n,1)   # 随机取一个1维列向量初始化theta\n",
    "\n",
    "# 初始化步长/学习率\n",
    "alpha = 0.001\n",
    "# 初始化误差，每个维度的theta都应该有一个误差，所以误差是一个4维。\n",
    "error = np.zeros((n, 1))  # 列向量\n",
    "\n",
    "# 初始化偏导数\n",
    "diff = np.zeros((input_data.shape[1], 1 ))\n",
    "\n",
    "# 初始化循环次数\n",
    "count = 0\n",
    "\n",
    "while count < loop_max:\n",
    "    count += 1\n",
    "    sum_m = np.zeros((n, 1))\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(input_data.shape[1]):\n",
    "            # 计算每个维度的theta\n",
    "            diff[j] = (input_data[i].dot(theta)-y_train[i])*input_data[i, j]\n",
    "        # 求每个维度的梯度的累加和\n",
    "        sum_m = sum_m + diff\n",
    "    # 利用这个累加和更新梯度    \n",
    "    theta = theta - alpha * sum_m\n",
    "    # else中将前一个theta赋值给error,theta - error便表示前后两个梯度的变化，当梯度\n",
    "    #变化很小（在接收的范围内）时，便停止迭代。\n",
    "    if np.linalg.norm(theta - error) < epsilon:\n",
    "        break\n",
    "    else:\n",
    "        error = theta\n",
    " \n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.55079689]\n",
      " [2.76039085]\n",
      " [2.4057708 ]\n",
      " [3.19224841]]\n"
     ]
    }
   ],
   "source": [
    "#小批量梯度算法\n",
    "import numpy as np\n",
    "\n",
    "# 构造训练数据集\n",
    "x_train = np.array([[2, 0., 3], [3, 1., 3], [0, 2., 3], [4, 3., 2], [1, 4., 4]])\n",
    "m = len(x_train)\n",
    "\n",
    "x0 = np.full((m, 1), 1)\n",
    "\n",
    "# 构造一个每个数据第一维特征都是1的矩阵\n",
    "input_data = np.hstack([x0, x_train])\n",
    "m, n = input_data.shape\n",
    "theta1 = np.array([[2 ,3, 4]]).T\n",
    "# 构建标签数据集，后面的np.random.randn是将数据加一点噪声，以便模拟数据集。\n",
    "#y_train = (input_data.dot(np.array([1, 2, 3, 4]).T)).T\n",
    "y_train = x_train.dot(theta1) + np.array([[2],[2],[2],[2],[2]]) \n",
    "\n",
    "# 设置两个终止条件\n",
    "loop_max = 10000\n",
    "epsilon = 1e-5\n",
    "\n",
    "# 初始theta\n",
    "np.random.seed(0)  # 设置随机种子\n",
    "theta = np.random.randn(n,1)   # 随机取一个1维列向量初始化theta\n",
    "\n",
    "# 初始化步长/学习率\n",
    "alpha = 0.0001\n",
    "# 初始化误差，每个维度的theta都应该有一个误差，所以误差是一个4维。\n",
    "error = np.zeros((n, 1))  # 列向量\n",
    "\n",
    "# 初始化偏导数\n",
    "diff = np.zeros((input_data.shape[1], 1 ))\n",
    "\n",
    "# 初始化循环次数\n",
    "count = 0\n",
    "\n",
    "# 设置小批量的样本数\n",
    "minibatch_size= 2 \n",
    "\n",
    "while count < loop_max:\n",
    "    count += 1\n",
    "    sum_m = np.zeros((n, 1))\n",
    "\n",
    "    for i in range(1, m, minibatch_size):\n",
    "        for j in range(i - 1, i + minibatch_size - 1, 1):\n",
    "            # 计算每个维度的theta\n",
    "            diff[j] = (input_data[i].dot(theta)-y_train[i])*input_data[i, j]\n",
    "        # 求每个维度的梯度的累加和\n",
    "        sum_m = sum_m + diff\n",
    "    # 利用这个累加和更新梯度    \n",
    "    theta = theta - alpha * (1.0 / minibatch_size)* sum_m\n",
    "    # else中将前一个theta赋值给error,theta - error便表示前后两个梯度的变化，当梯度\n",
    "    #变化很小（在接收的范围内）时，便停止迭代。\n",
    "    if np.linalg.norm(theta - error) < epsilon:\n",
    "        break\n",
    "    else:\n",
    "        error = theta\n",
    "    \n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降算法的调优方法（目的：加快收敛速度）\n",
    "\n",
    "当选择好了使用BGD、SGD、MBGD其中一个梯度下降方式后，对梯度下降算法需要进行调优，那么应该从哪些方面进行调优？\n",
    "\n",
    "**1 学习速率(Learning Rate)α调优**\n",
    "\n",
    " 在$\\theta$迭代结算公式中，其中的偏导数的系数$a$是学习速率（Learning Rate），且α>0。\n",
    "\n",
    "1）固定的$a$，$a$太大的话，导致迭代次数变少(因为$\\theta$增量变大)，学习速率变快，训练快。但是$a$不是越大越好，如果$a$太大的话，会导致梯度下降算法在图形的上坡和下坡上面来回震荡计算，严重的结果可能无法收敛；\n",
    "\n",
    "2）固定的$a$，$a$太小的话，导致迭代次数变多(因为$\\theta$增量变小)，学习速率变慢，训练慢。但是$a$不是越小越好，如果$a$太小的话，会导致梯度下降算法在图形迭代到最优点处整个过程需要训练很长时间，导致训练太慢，虽然可以取得最优$\\theta$。\n",
    "\n",
    "3）变化的$a$，当梯度大的时候，学习速率变大，梯度小的时候，学习速率变小。则学习速率和梯度是一个正相关，可以提高下降算法的收敛速度。$a$和梯度的正相关有一个比例系数，称为Fixed Learning Rate。Fixed Learning Rate一般取0.1或者0.1附近的值，可能不是最好但是一定不会太差。如公式（3）就是动态变化的步长。\n",
    "\n",
    "**2选取最优的初始值$\\theta$**\n",
    "首先，初始值$\\theta$不同，获得的代价函数的最小值也可能不同，因为每一步梯度下降求得的只是当前局部最小而已。所以需要多次进行梯度下降算法训练，每次初始值θ都不同，然后选取代价函数取得的最小值最小的那组初始值$\\theta$。\n",
    "\n",
    "**3特征数据归一化处理**\n",
    "\n",
    " 样本不相同，特征值的取值范围也一定不同。特征值的取值范围可能会导致迭代很慢。所以就要采取措施减少特征值取值范围对迭代的影响，这个措施就是对特征数据归一化。\n",
    "\n",
    "数据归一化方法有：1）线性归一化，2）均值归一化。一般图像处理时使用线性归一化方法，比如将灰度图像的灰度数据由[0,255]范围归一化到[0,1]范围。如果原始数据集的分布近似为正态（高斯）分布，那么可以使用均值归一化对数据集进行归一化，归一化为：均值为0，方差为1的数据集。\n",
    "\n",
    "经过特征数据归一化后，梯度下降算法会在期望值为0，标准差为1的归一化特征数据上进行迭代计算$\\theta$，这样迭代次数会大大加快\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降应用\n",
    "下表为房屋面积和价格，属于线性关系。试预测房屋为137平米时的价格。要求使用梯度下降模型。\n",
    "\n",
    "实现中借用了上述随机梯度下降的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>面积（S/平米）</th>\n",
       "      <th>价格（P/万元）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140.0</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.5</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155.0</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160.0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>170.0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>177.0</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187.5</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>235.0</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>245.0</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   面积（S/平米）  价格（P/万元）\n",
       "0     110.0       180\n",
       "1     140.0       245\n",
       "2     142.5       239\n",
       "3     155.0       245\n",
       "4     160.0       262\n",
       "5     170.0       279\n",
       "6     177.0       280\n",
       "7     187.5       308\n",
       "8     235.0       405\n",
       "9     245.0       394"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "house={\n",
    "    '面积（S/平米）':[110,140,142.5,155,160,170,177,187.5,235,245],\n",
    "    '价格（P/万元）':[180,245,239,245,262,279,280,308,405,394]\n",
    "}\n",
    "#定义数据库\n",
    "houseDf=pd.DataFrame(house)\n",
    "houseDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>面积</th>\n",
       "      <th>价格</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>[0.28888888888888875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240741</td>\n",
       "      <td>[0.26222222222222213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>[0.28888888888888875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.370370</td>\n",
       "      <td>[0.36444444444444435]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>[0.43999999999999995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.496296</td>\n",
       "      <td>[0.4444444444444444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.574074</td>\n",
       "      <td>[0.5688888888888888]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.925926</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.951111111111111]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         面积                     价格\n",
       "0  0.000000                  [0.0]\n",
       "1  0.222222  [0.28888888888888875]\n",
       "2  0.240741  [0.26222222222222213]\n",
       "3  0.333333  [0.28888888888888875]\n",
       "4  0.370370  [0.36444444444444435]\n",
       "5  0.444444  [0.43999999999999995]\n",
       "6  0.496296   [0.4444444444444444]\n",
       "7  0.574074   [0.5688888888888888]\n",
       "8  0.925926                  [1.0]\n",
       "9  1.000000    [0.951111111111111]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机梯度下降法\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#准备数据\n",
    "x_train= np.ones([houseDf.shape[0],2])\n",
    "# 构造一个每个数据第一维特征都是1的矩阵\n",
    "x_train[:,1] = min_max_scaler.fit_transform(np.array(houseDf.iloc[:,0]).reshape(-1,1)).T#数据较大，需要做归一化处理\n",
    "y_train=min_max_scaler.fit_transform(np.array(houseDf.iloc[:,1]).reshape(-1,1))#表中的价格\n",
    "m, n = x_train.shape\n",
    "pd.DataFrame({\"面积\":list(x_train[:,1]),\"价格\":list(y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.22222222, 0.24074074, 0.33333333, 0.37037037,\n",
       "       0.44444444, 0.4962963 , 0.57407407, 0.92592593, 1.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是线性关系，设模型为$y=w_0+w_1X$，优化目标为误差平方和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00539135 0.98867551]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x256ec99be48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x256ec970e08>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置两个终止条件\n",
    "loop_max =10000\n",
    "epsilon = 1e-6\n",
    "\n",
    "# 初始化theta（权重）\n",
    "np.random.seed(0)\n",
    "theta = np.random.rand(n).T # 随机生成10以内的，n维1列的矩阵\n",
    "\n",
    "# 初始化步长/学习率\n",
    "alpha = 0.001\n",
    "# 初始化迭代误差（用于计算梯度两次迭代的差）\n",
    "error = np.zeros(n)\n",
    "\n",
    "# 初始化偏导数矩阵\n",
    "diff = np.zeros(n)# 初始化迭代误差（用于计算梯度两次迭代的差）\n",
    "#print(diff)\n",
    "# 初始化循环次数\n",
    "count = 0\n",
    "\n",
    "while count < loop_max:\n",
    "    count += 1  # 每运行一次count加1，以此来总共记录运行的次数\n",
    "    \n",
    "    # 计算梯度\n",
    "    for i in range(m):\n",
    "        # 计算每个维度theta的梯度，并运用一个梯度去更新它\n",
    "        diff = x_train[i].dot(theta)-y_train[i]\n",
    "        #print(diff)\n",
    "        theta = theta - alpha * diff*(x_train[i])\n",
    "    #print(diff,theta)\n",
    "    \n",
    "    # else中将前一个theta赋值给error,theta - error便表示前后两个梯度的变化，当梯度\n",
    "    #变化很小（在接收的范围内）时，便停止迭代。\n",
    "    if np.linalg.norm(theta - error) < epsilon: # 判断theta与零向量的距离是否在误差内\n",
    "        break\n",
    "    else:\n",
    "        error = theta  \n",
    "        \n",
    "print(theta)\n",
    "plt.scatter(x_train[:,1],y_train)\n",
    "pred_y= theta[1]*x_train[:,1]+theta[0]\n",
    "plt.plot(x_train[:,1],pred_y,c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta设置为随机且循环1000次时，结果为[0.11206335 0.77391136]，循环10000次时，结果为[0.00539135 0.98867551]。设回归模型为y=0.00540+0.9986x，价格137归一化为0.2，带入回归模型，输出归一化结果为0.505，最终价格为292.5。从图形来看，回归模型的结果是正确的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 牛顿迭代法\n",
    "### 基本原理及案例代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "牛顿迭代法，是一种快速迭代搜索法，主要用于求函数零点，即求方程的根。该算法要求目标函数具有二阶连续偏导数，因为下一个近似值需要通过在现有近似值附近进行一阶泰勒展开来确定。由微积分理论可知，任意$n$阶可导的函数都可以在任意点$X_k$处展开为幂函数形式，如下：$$f(X)=\\sum_{n=0}^{\\infty}\\frac{f^{(n)}(X_k)}{n!}(X-X_k)^n \\tag 1$$其中，$f^{(n)}(X_k)$为点$X_k$处的$n$阶导数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$f(X)=0$方程，如果是非线性方程，则取式（1）的线性部分，即$f(X)=f(X_k)+f^{'}(X_k)(X-X_k)=0$，若$f^{'}(X_k)\\neq 0$，则可以得到$f(X)=0$的一个近似根，即$X=X_k-\\frac{f(X_k)}{f^{'}(X_k)}$，将这个近似根作为新的起点，则可以得到迭代公式：$$X_{k+1}=X_k-\\frac{f(X_k)}{f^{'}(X_k)}$$如果迭代初始值$X_0$选择得当，则可通过迭代得到方程$f(X)=0$的根为极限的收敛序列$\\{X_k\\}$。当$k$足够大时，可获得满足精度要求的方程近似根$X_k$。\n",
    "\n",
    "迭代求函数根的原理可以通过下图说明，其中假设最优解为$r$：![jupyter](./img/model-3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于函数优化问题，目标函数的极值点为函数驻点，即为函数导函数的根。牛顿迭代法就可以求得导函数的根，由此获得目标函数的极值点。若$f(X)$为$F(X)$的导函数，则当$f(X)=0$时，$F(X)$在$X$处有极值点。\n",
    "\n",
    "假设目标函数具有三阶导数，且$F''(X)\\neq 0$，同理可得如下迭代式：$$X_{k+1}=X_k-\\frac{F'(X_k)}{F''(X_k)}$$适当选择$X_0$，可使用上式收敛到$F'(X)=0$的根，即目标函数$F(X)$的极值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于机器学习的目标函数或者代价函数，一般包含多个模型参数，使用牛顿迭代法，相当于求解多元目标函数的极小值点。故需要**将一元函数的迭代法推广到多元函数的情形。**\n",
    "\n",
    "设$F(X)$为三次可微的$n$元函数，则由多元函数在$X_k$处泰勒展开，$$F(X)\\approx F(X_k)+\\nabla F(X_k)\\cdot(X-X_k)+\\frac{1}{2}(X-X_k)^T\\nabla ^2F(X_k)\\cdot(X-X_k) \\tag 2$$其中，$X=(x_1,x_2,\\dots,x_n)^T$，$F(X)$在$X=X_k$处的一阶导数$$\\nabla F(X_k)=\\left (\\frac{\\partial F}{\\partial x_1},\\frac{\\partial F}{\\partial x_2},\\dots,\\frac{\\partial F}{\\partial x_n}\\right )^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$𝐹(𝑋)$在$𝑋=𝑋_𝑘$处的二阶导是一个Hesse矩阵：\n",
    "$$\\nabla F(X_k)=\n",
    "\\begin{eqnarray*}\\left (\\matrix{\n",
    "\\frac{\\partial^2F}{\\partial x_1^2}&\\frac{\\partial^2F}{\\partial x_1\\partial x_2}&\\dots& \\frac{\\partial^2F}{\\partial x_1\\partial x_n} \\\\\n",
    "\\frac{\\partial^2F}{\\partial x_2\\partial x_1}&\\frac{\\partial^2F}{\\partial x_2^2}&\\dots& \\frac{\\partial^2F}{\\partial x_2\\partial x_n}\\\\\n",
    "\\cdots \\\\\n",
    "\\frac{\\partial^2F}{\\partial x_n\\partial x_1}&\\frac{\\partial^2F}{\\partial x_n\\partial x_2}&\\dots& \\frac{\\partial^2F}{\\partial x_n^2}}\n",
    "\\right ) \\end{eqnarray*}\\tag 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定式（2）右边为$n$元正定二次凸函数，且存在唯一的最优解，则对式（2）求极值，可将$\\nabla F(X)=0$近似表示成$$\\nabla F(X)\\approx \\nabla F(X_k)+\\nabla^2F(X_k)\\cdot (X-X_k)=0$$求解迭代式为：$$X_{k+1}=X_k-[\\nabla^2F(X_k)]^{-1}\\nabla F(X_k)\\tag 4$$可将其看成步长恒为1，方向为$P_k=-[\\nabla^2F(X_k)]^{-1}\\nabla F(X_k)$的迭代搜索。这个方向称为**牛顿方向**。\n",
    "\n",
    "上述上述分析，可得牛顿迭代法的具体步骤：\n",
    "\n",
    "1）设置初始点$X_k$和终止准则，并置$X_k=0$\n",
    "\n",
    "2）求解点$X_k$对应的目标函数、梯度和Hesse矩阵\n",
    "\n",
    "3）确定牛顿方向$P_k$\n",
    "\n",
    "4）根据公式（4）确定下一个点$X_{k+1}$\n",
    "\n",
    "5）判断终止条件，确定最优解或者转到步骤（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：下表式一个预测广告投入与净利润之间关系的机器模型，使用该模型预测广告投入为2.1万元时的净利润。要求模型优化过程采用牛顿迭代法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>广告投入X</th>\n",
       "      <th>净利润y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.69</td>\n",
       "      <td>12.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.41</td>\n",
       "      <td>11.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.47</td>\n",
       "      <td>12.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.43</td>\n",
       "      <td>11.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.39</td>\n",
       "      <td>11.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.15</td>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.54</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.67</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.24</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.77</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.46</td>\n",
       "      <td>12.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.83</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.15</td>\n",
       "      <td>12.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.15</td>\n",
       "      <td>12.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    广告投入X   净利润y\n",
       "0    4.69  12.23\n",
       "1    6.41  11.84\n",
       "2    5.47  12.25\n",
       "3    3.43  11.10\n",
       "4    4.39  11.97\n",
       "5    2.15   8.75\n",
       "6    1.54   7.75\n",
       "7    2.67  10.50\n",
       "8    1.24   6.71\n",
       "9    1.77   7.60\n",
       "10   4.46  12.46\n",
       "11   1.83   8.47\n",
       "12   5.15  12.27\n",
       "13   6.15  12.02"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pred={\n",
    "    '广告投入X':[4.69,6.41,5.47,3.43,4.39,2.15,1.54,2.67,1.24,1.77,4.46,1.83,5.15,6.15],\n",
    "    '净利润y':[12.23,11.84,12.25,11.10,11.97,8.75,7.75,10.50,6.71,7.60,12.46,8.47,12.27,12.02]\n",
    "}\n",
    "#定义数据库\n",
    "predDf=pd.DataFrame(pred)\n",
    "predDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x256ec99bc88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x256ec9d0848>,\n",
       " <matplotlib.lines.Line2D at 0x256eca0b508>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#数据可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X=predDf.iloc[:,0]#提取两列数据，分别作为x,y\n",
    "y=predDf.iloc[:,1]\n",
    "plt.scatter(X,y)\n",
    "\n",
    "# 生成拟合曲线的绘制点\n",
    "_X = np.arange(1, 7, 0.1) \n",
    "_Y = np.array([-0.145 + 5.313*x - 0.556*x**2 for x in _X])\n",
    " \n",
    "plt.plot(X, y, 'ro', _X, _Y, 'b', linewidth=2) \n",
    "#plt.title(\"y = {} + {}x + {}$x^2$ \".format(a0, a1, a2)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据散点图可以看出，可用二次函数拟合表中的数据。设学习模型为$y=w_0+w_1X+w_2X$，用误差平方和作为优化目标，为便于计算，优化目标函数定义为误差平方和的$\\frac{1}{2}$。即$$E[W]=\\frac{1}{2}\\sum_{i=1}^n(y^i-y_p^i)=\\frac{1}{2}\\sum_{i=1}^n(y^i-w_2X^{i2}-w_1X^{i}-w_0)$$其中，$y^i$、$y_p^i$分别为第$i$个数据的真实值和预测值。\n",
    "\n",
    "代入数据，得到目标函数$F(W)=F(w_0,w_1w_2)$,如设置初始点$W_0=(1,1,1)$，可以进一步求得$\\nabla F(W_0)、\\nabla F(w_0)$。 使用公式（4）不断迭代，满足结束条件时，得到模型$y=-0.556X^2+5.313X-0.145$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 牛顿迭代法的优缺点\n",
    "该部分参考：https://my.oschina.net/u/3579120/blog/1508433\n",
    "\n",
    "牛顿法优点：二阶收敛，收敛速度快；\n",
    "\n",
    "牛顿法缺点：\n",
    "\n",
    "1 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。\n",
    "\n",
    "2 可能发生被零除错误。当函数在它的零点附近，导函数的绝对值非常小时，运算会出现被零除错误。\n",
    "\n",
    "3 是可能出现死循环。当函数在它的零点有拐点时，可能会使迭代陷入死循环。\n",
    "\n",
    "4 定步长迭代。改进是阻尼牛顿法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型正则化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习的过程中，假设的模型空间有众多模型，不同的模型有不同的复杂度（例如，不同的个数），机器学习任务jiu是要选择逼近真实的模型。\n",
    "\n",
    "如果一味追求提高对训练数据的预测能力，则选择的模型复杂度往往会比真实模型更高，出现训练误差较小而泛化误差较大的情形，这就是**过拟合(overfiting / high variance)现象**。也即，过拟合表现为模型在训练集上表现很好，但是在测试集上表现较差，模型的泛化能力弱。 一般而言，过拟合就是模型对训练数据的信息提取过多，不仅学习到了数据背后的规律，连数据噪声都当做规律学习了，以至于出现这一模型对已知数据预测的很好，但是对未知数据预测很差。机器学习的**模型选择问题**旨在避免过拟合并提高模型的预测能力。 \n",
    "\n",
    "产生过拟合的原因很多，主要表现在以下几个方面：\n",
    "\n",
    "    1）数据样本抽取错误。如样本数量太少，选择方法错误，样本标签错误等。导致选取的样本数据不足以代表预定的分类规则。\n",
    "    2）数据太多，模型复杂度过高。\n",
    "    3）样本噪音干扰过大。导致误认为是特征而干扰了分类规则。\n",
    "    4）权值学习迭代次数过多。这样拟合了噪声和没有代表性的特征。\n",
    "    \n",
    "正则化策略是机器学习中十分常用的防止模型过拟合，提高模型的泛化能力的技术。其目的就是以增大训练误差为代价，来减少测试误差（如果在训练误差上很小，可能出现过拟合的情况）。正则化思想是在损失函数中加入刻画模型复杂程度的指标，这个指标一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。\n",
    "\n",
    "下面介绍几种常用的正则化策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数约束\n",
    "通过对模型参数添加惩罚参数来限制模型能力，常用的就是在损失函数基础上添加范数约束。\n",
    "\n",
    "通常情况下，深度学习中只对仿射参数添加约束，**对偏置项不加约束**。主要原因是偏置项一般需要较少的数据就能精确的拟合。如果添加约束常常会导致欠拟合。\n",
    "\n",
    "范数约束一般分为：L1、L2 正则化，是通过在损失函数中添加一项对权重的约束来实现正则化的，L1 是求权重的绝对值之和，L2 是求权重的平方和。\n",
    "\n",
    "具体来说，假设有训练样本集$S={(X_1,y_1),(X_2,y_2),\\cdots,(X_n,y_n)}$，模型仿射参数$\\beta=(\\beta_1,\\beta_2,\\cdots,\\beta_n)^T$，模型训练的误差函数为$$F(\\beta)=\\frac{1}{n}\\sum_{X_i\\in S}L(f(X_i)-y_i)$$\n",
    "\n",
    "在损失函数上加上约束$a\\lambda (\\beta)$，得到新的目标函数$$F'(\\beta)=\\frac{1}{n}\\sum_{X_i\\in S}L(f(X_i)-y_i)+a\\lambda (\\beta)$$约束项$a\\lambda (\\beta)$中的$a$为大于0的超参数，为调整经验风险和正则化项之间关系的系数，值越大，对参数惩罚越严重。$\\lambda (\\beta)$是对参数向量的**惩罚形式**。\n",
    "\n",
    "**$L^1$范数约束**\n",
    "\n",
    "1-范数为$║\\beta║_1=│\\beta_1│+│\\beta_2│+…+│\\beta_n│$。模型越复杂，正则化权重$a$应该设置的越大。\n",
    "\n",
    "可用梯度下降法最小化目标函数$F'(\\beta)$，得到$$\\nabla F'(\\beta)=\\frac{1}{n}\\sum_{X_i\\in S}\\nabla _{\\beta}L(f(X_i)-y_i)+asign(\\beta)$$\n",
    "其中，$sign(x)$是符号函数，对于任意的$\\beta_i$，有\n",
    "$$sign(\\beta_i)=\n",
    "\\begin{eqnarray*}\\left \\{\\matrix{\n",
    "1,\\beta_i>0\\\\\n",
    "0,\\beta_i=0\\\\\n",
    "-1,\\beta_i<0\n",
    "}\\right.  \\end{eqnarray*}$$\n",
    "参数更新公式如下：$$\\beta^{k+1}=\\beta^k-\\epsilon a sign(\\beta^k)-\\epsilon \\nabla F(\\beta^k)$$\n",
    "\n",
    "公式右侧的中间项是关键部分，对参数的取值做了一定的控制，即**使参数尽量向0靠拢**。因为，当$\\beta_i>0$,减去了$|\\epsilon a sign(\\beta^k)|$，$\\beta_i<0$，增加了中间项$|\\epsilon a sign(\\beta^k)|$。**因此，1-范数的基本目的是尽量产生稀疏的参数向量，即使得更多的参数为0**。这也意味着，降低了模型对数据的拟合能力，由此可以缓解过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$L^2$范数约束**\n",
    "\n",
    "2-范数为$║\\beta║_2=(\\beta_1^2+\\beta_2^2+…+\\beta_n^2)^{\\frac{1}{2}}$。约束为2-范数时，新的目标函数$$F'(\\beta)=\\frac{1}{n}\\sum_{X_i\\in S}L(f(X_i)-y_i)+a\\lambda (\\beta)=\\frac{1}{n}\\sum_{X_i\\in S}L(f(X_i)-y_i)+\\frac{a}{2}║\\beta║_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用梯度下降法对目标函数最小值进行优化，得到$F'(\\beta)$的梯度：$$\\nabla F'(\\beta)=\\nabla F(\\beta)+a\\beta$$得到新的迭代公式$$\\beta^{k+1}=\\beta^k-\\epsilon[a\\beta^k+\\nabla F(\\beta^k)]=(1-\\epsilon a)\\beta^k-\\epsilon \\nabla F(\\beta^k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "右边第一项比原参数减少了$\\frac{1}{\\epsilon a}$，故这种方法通常称为**权重衰减**。可以看到，2-范数约束并没有刻意促使某些参数为0，而是使得模型的所有参数都变小。\n",
    "\n",
    "从优化或者数值计算的角度来说，如目标函数为二次函数，使用最小二乘法可以直接求解$$\\hat \\beta=(X^TX)^{-1}X^Ty$$但是若参数过多，特征维数大于样本数量时，上式无法求逆，难以直接求解。但是使用L2范数作为约束项后，目标函数转化为$$F(\\beta)=(y-X\\beta)^T(y-X\\beta)+\\frac{1}{2}a\\beta^T\\beta$$可以得到最优化的具体表现形式$$\\hat \\beta=(X^TX+aE)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本增强\n",
    "\n",
    "数据增强，也就是获取和使用更多的数据集。给与模型足够多的数据集，让它在尽可能多的数据上进行“观察”和拟合，从而进行不断修正。样本增强是一种基于扩充样本训练集的正则化策略，我们不可能收集无限多的数据集，所以通常的方法，就是对已有的数据添加大量的“噪音”，产生若干虚拟样本，比如图像，可以对其进行锐化、旋转、明暗度调整等。 \n",
    "\n",
    "样本增强产生的虚拟样本必须具有一定的合理性，既要与现有样本保持一定的差异，又要服从一致的总体样本分布。\n",
    "\n",
    "样本增强常用在图像领域，原因在于图像包含很多不改变原始信息表达的可变因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估与选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估方法\n",
    "若m个样本中有a个样本分类错误，则有下面的概念：\n",
    "\n",
    "**错误率error rate**：分类错误的样本数占样本总数的比例$E=a/m$。\n",
    "\n",
    "**精度accuracy**：分类正确的样本数占样本总数的比例$1-E$。\n",
    "\n",
    "**误差error**：｜实际预测输出-样本真实输出｜。\n",
    "\n",
    "**训练误差training error或经验误差empirical error**：学习器在训练集上的误差。\n",
    "\n",
    "**泛化误差generalization error**：在新样本上的误差。\n",
    "\n",
    "学习的理想情况：从训练样本中尽可能学出适用于所有潜在样本的“普通规律”。如果学习器把训练样本学的“太好”，很可能导致已将训练样本自身一些特点当作了潜在样本的共性，如些导致泛化性能下降，这种现象称为**过拟合（overfitting）**，主要是由于学习能力太强，导致把训练样本中的不太一般的特性都学到了。\n",
    "\n",
    "如果训练器学习能力低下,对训练样本的一般性质未学好，则就是欠拟合underfitting。\n",
    "\n",
    "欠拟合比较容易解决，过拟合则不太好解决，过拟合是机器学习面临的关键障碍，但过拟合是无法彻底避免的。\n",
    "\n",
    "现实学习任务中，选择学习算法及确定参数配置即是“模型选择”（model selection）问题。理想的解决方案是对候选模型的**泛化误差**进行评估，选择泛化误差最小的模型。然而，jiqixuexi 无法直接获得泛化误差，在现实中，如何进行模型评估而选择模型呢？\n",
    "\n",
    "由于无法获取所有样本上的泛化误差，因此一般只能以测试集上的“测试误差”（testing error）作为泛化误差的近似。测试集应尽可能与训练集互斥，即测试样本未在训练中使用过。\n",
    "\n",
    "针对m个样本的数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，如何划分出训练集S和测试集T？下面是几种常见的方法。\n",
    "\n",
    "（1）**留出法hold-out**，将数据集直接划分成训练集S和测试集T，为保持划分出的数据集的数据分布的一致性，在分类任务中要保持样本类别比例相似，这种保留类别比例的采样方式一般称为“分层采样”（stratified sampling）,例如D共含500个正例，500个反例，分层采样30%作为测试样本，则S应包含350个正例，350个反例，T应包含150个正例，150个反例。\n",
    "\n",
    "留出法有很多种划分S和T的方法，单次使用留出法，一般要采用若干次随机划分，重复实验评估后取平均值作为评估结果。S太大则训练出的模型更接近D，评估结果具有较好保真性（fidelity），但测试集T太小，评估结果不够稳定准确，反之评估结果的保真性又得不到保证，因此没有完美解决方案，常见做法约2/3~4/5的样本用于训练，剩余用于测试。\n",
    "\n",
    "（2）**交叉验证法**，$D=D_1∪D_2∪…∪D_k，Di∩Dj=ϕ（i≠j）$，$D_i$均是分层采样，各子集互斥，每次用$k-1$个子集的并集作为训练集，余下那个子集作为测试集，可获得k组训练/测试集，可进行k次训练和测试，最终返回k个测试结果均值。交叉验证评估结果稳定性及保真性很大程度上取决于k的取值，通过称为**k折交叉验证**（k-fold cross validation），通常取10，或5、20。k组训练/测试集存在许多种划分方式，k折交叉验证通常随机使用不同划分重复p次，最终评估结果是这p次k折交叉验证结果的均值。\n",
    "\n",
    "假定包含m个样本，令k=m，则得到交叉验证法的特例留一法（Leave-One-Out,简称LOO）。留一法的评估结果往往被认为较准确，但数据集较大时，计算开销难以忍受。\n",
    "\n",
    "（3）**自助法bootstrapping**，因留出法与交叉验证法均要留一部分样本用于测试，因此评估模型用的训练集比D小，必然引入一些因训练样本规模不同导致的估计偏差（留一法受此影响最小，但计算复杂度太高了）。自助法是个较好的解决方法，它以自助采样法（bootstrap sampling）为基础，即对于容量为m的样本集D，采用有放回的采样方式，从中采取m个样本形成数据集D’，显然其中可能存在重复样本，经推算初始数据集D约有36.8%的样本未出现在D’中，于是将D’作为训练集，而D\\D’作为测试集。样本始终不被采样到的概率计算$$(1-\\frac{1}{m})^m=\\frac{1}{m}=0.368$$\n",
    "\n",
    "自助法适用于数据集较小、难以有效划分训练/测试集时很有用。由于自助法产生的数据集改变了 初始数据集的分布，会引入估计偏差，因此，在初始数据量足够时，留出法和交叉验证法更常用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型性能度量\n",
    "对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是**性能度量**。\n",
    "\n",
    "给定训练集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，设有学习器$f$，要评价$f$的性能，就要将预测结果$f(x)$和真实标记$y$进行比较。\n",
    "\n",
    "如均方误差$$E(f;D)=\\frac{1}{m}\\sum_1^m(f(x_i)-y_i)^2$$\n",
    "\n",
    "对于数据分布D和概率密度函数$p(.)$，均方误差可描述为\n",
    "$$E(f;D)=\\int _{x\\sim D}(f(x)-y)^2p(x)dx$$\n",
    "对于分类问题，常用的度量标准如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（1）错误率与精度**\n",
    "错误率与精度自然就是算比例，最简单。错误率和精度分别定义如$$E(f;D)=\\frac{1}{m}\\sum_1^m\\amalg (f(x_i)\\neq y_i)$$\n",
    "\n",
    "$$acc(f;D)=1-E(f;D)=\\frac{1}{m}\\sum_1^m\\amalg (f(x_i)= y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)查准率、查全率与F1**\n",
    "查准率、查全率等在信息检索、web搜索等应用中经常出现。设有分类结果的混淆矩阵confusion matrix，表示为下表，表中左半部分预测类别为正例，右半部分预测类别为反例，而样本中真实的正例类别在上半部分，下半部分为真实的反例。 \n",
    "\n",
    "|真实情况|预测结果正例|预测结果反例|\n",
    "|---|---|---|\n",
    "|正例|TP（真正例）|FN（反正例）|\n",
    "|反例|FP（假正例）|TN（真反例）|\n",
    "\n",
    "所谓的查准率P和查全率R分别定义为：$$P=\\frac{TP}{TP+FP}$$\n",
    "$$R=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "查准率也就是在所有预测结果为正例的情况下的真实比例。查全率是所有真实情况为正例的情况下预测正确的比例。所以它适用于二分类问题。\n",
    "\n",
    "查准率和查全率是一对矛盾的度量。查准率的提高可以通过加大数据量来提高，但如此一来查全率就低了（分母大了）；同理，查全率高的时候，查准率往往偏低。在很多情况下，我们以查准率为纵轴，查全率为横轴作图，绘制P-R曲线。如图所示：![jupyter](./img/model-4.jpg)\n",
    "\n",
    "当一个模型a的P-R曲线完全包住另一个模型b的P-R曲线时，即可认为a优于b。如果两个模型的曲线有交叉，则难以断定优劣。如果确实要比较孰优孰劣，则比较曲线下的面积，但是面积难以估算。\n",
    "\n",
    "所以一般会综合两方面考量学习器的好坏，找到**最佳平衡点BEP**（Break-Even Point）。衡点定义是查全率等于查准率时的取值。BEP过于简化，更常用的是F1变量，本质上是P和R的调和平均。 \n",
    "$$F1=\\frac{2\\times P\\times R}{P+R}=\\frac{2\\times TP}{样例总数+TP-TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）ROC与AUC**\n",
    "\n",
    "在混淆矩阵confusion matrix中，样本中的真实正例类别总数即$TP+FN$。TPR即True Positive Rate，$$TPR = \\frac{TP}{TP+FN}$$ \n",
    "同理，样本中的真实反例类别总数为$FP+TN$。FPR即False Positive Rate，$$FPR=\\frac{FP}{TN+FP}$$\n",
    "\n",
    "还有一个概念叫**“截断点”**。机器学习算法对测试样本进行预测后，可以输出各样本对某个类别的相似度概率。“最有可能”是正例的排在前面，“最不可能”是正例的排在后面，分类过程就是就这个排序表中以某个截断点将样本分为两部分，前一部分位正例，后一部分为负例。\n",
    "\n",
    "截断点取不同的值，TPR和FPR的计算结果也不同。将截断点不同取值下对应的TPR和FPR结果画于二维坐标系中得到的曲线，就是**ROC曲线**，全称是“受试者工作特征”( Receiver Operating Cha\\fracteristic Curve )，描述的 TPR与 FPR 之间关系的曲线。横轴用FPR表示。如下图![jupyter](./img/model-5.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际应用中，设定不同的截断点，得到若干离散点对（假正例率，真正例率），从而绘制出ROC曲线。使用不同的模型，得到不同的ROC曲线。若一个ROC曲线被另外一个ROC曲线包住，则后者的性能优于前者；如果有交叉，则较为合理的判据是比较ROC曲线下的面积，即AUC，很明显，模型的 AUC 越高，模型的二分类性能就越强。假设ROC曲线（上图右）有m个离散点，则AUC 的计算公式如下：$$AUC=\\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)其他方法**\n",
    "\n",
    "    代价敏感错误率和代价曲线：略\n",
    "    比较检验：略\n",
    "    偏差与方差：\n",
    "    \n",
    "偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，**偏差**指的是预测的期望值与真实值的偏差，**方差**则是每一次预测值与预测值的期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到：\n",
    "\n",
    "    •期望泛化误差=方差+偏差 \n",
    "    •偏差刻画学习器的拟合能力\n",
    "    •方差体现学习器的稳定性\n",
    "\n",
    "易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。[jupyter](./img/model-6.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
