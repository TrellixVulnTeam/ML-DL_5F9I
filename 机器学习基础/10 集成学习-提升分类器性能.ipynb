{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的文章已经介绍了几种种不同的分类器，它们各有优缺点。我们可以很自然地将不同的分类器组合起来，而这种组合结果则被成为**集成方法(ensemble method)或者元算法(meta-algorithm)**。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一种算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。\n",
    "\n",
    "使用到的数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成方法\n",
    "集成方法（ensemble method）通过组合多个学习器来完成学习任务，颇有点“三个臭皮匠顶个诸葛亮”的意味。基分类器一般采用的是**弱可学习（weakly learnable）分类器**，通过集成方法，组合成一个**强可学习（strongly learnable）分类器**。所谓弱可学习，是指学习的正确率仅略优于随机猜测的多项式学习算法；强可学习指正确率较高的多项式学习算法。集成学习的泛化能力一般比单一的基分类器要好，这是因为大部分基分类器都分类错误的概率远低于单一基分类器的。\n",
    "\n",
    "集成方法主要包括**Bagging和Boosting两种方法**，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法，即将弱分类器组装成强分类器的方法\n",
    "\n",
    "## Bagging\n",
    "**自举汇聚法（bootstrap aggregating），也称为bagging方法**。Bagging对训练数据采用自举采样（boostrap sampling），即有放回地采样数据，**主要思想：**\n",
    "从原始样本集中抽取训练集。每轮从原始样本集中使用**Bootstraping方法**抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "\n",
    "每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "\n",
    "对分类问题：将上步得到的k个模型采用**投票**的方式得到分类结果；对回归问题，计算上述模型的**均值**作为最后的结果。（所有模型的重要性相同）![jupyter](./AdaBoost-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting是一种与Bagging很类似的技术。Boosting的思路则是采用重赋权（re-weighting）法迭代地训练基分类器，**主要思想**：每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果。基分类器之间采用序列式的线性加权方式进行组合。![jupyter](./AdaBoost-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging、Boosting二者之间的区别\n",
    "**样本选择上：**\n",
    "\n",
    "Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "\n",
    "Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "**样例权重：**\n",
    "\n",
    "Bagging：使用均匀取样，每个样例的权重相等。\n",
    "\n",
    "Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "**预测函数：**\n",
    "\n",
    "Bagging：所有预测函数的权重相等。\n",
    "\n",
    "Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "**并行计算：**\n",
    "\n",
    "Bagging：各个预测函数可以并行生成。\n",
    "\n",
    "Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n",
    "\n",
    "## 总结\n",
    "这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n",
    "\n",
    "下面是将决策树与这些算法框架进行结合所得到的新的算法：\n",
    "Bagging + 决策树 = **随机森林**\n",
    "AdaBoost + 决策树 = **提升树**\n",
    "Gradient Boosting + 决策树 = **GBDT**\n",
    "集成方法众多，本文主要关注Boosting方法中的一种最流行的版本，即AdaBoost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "AdaBoost算法是基于Boosting思想的机器学习算法，AdaBoost是**adaptive boosting**（自适应boosting）的缩写，其运行过程如下：\n",
    "\n",
    "**1、计算样本权重**\n",
    "\n",
    "训练数据中的每个样本，赋予其权重，即样本权重，用向量$D$表示，这些权重都初始化成相等值。假设有n个样本的训练集：$\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}$设定每个样本的权重都是相等的，即1/n。\n",
    "\n",
    "**2、计算错误率**\n",
    "\n",
    "利用第一个弱学习算法h1对其进行学习，学习完成后进行错误率$\\epsilon$的统计：$\\epsilon=\\frac{未正确分类的样本数}{所有样本数}$\n",
    "\n",
    "**3、计算弱学习算法权重**\n",
    "\n",
    "弱学习算法也有一个权重，用向量α表示，利用错误率计算权重$a=\\frac{1}{2}ln \\frac{1-\\epsilon}{\\epsilon}$\n",
    "\n",
    "**4、更新样本权重**\n",
    "\n",
    "在第一次学习完成后，需要重新调整样本的权重，以使得在第一分类中被错分的样本的权重，在接下来的学习中可以重点对其进行学习：![jupyter](./AdaBoost-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5、AdaBoost算法**\n",
    "\n",
    "重复进行学习，这样经过t轮的学习后，就会**得到t个弱学习算法、权重、弱分类器的输出以及最终的AdaBoost算法的输出**，分别如下：![jupyter](./AdaBoost-4.png)其中，sign(x)是符号函数。具体过程如下所示：![jupyter](./AdaBoost-5.png)AdaBoost算法总结如下：![jupyter](./AdaBoost-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于单层决策树构建弱分类器\n",
    "建立AdaBoost算法之前，我们必须先建立弱分类器，并保存样本的权重。弱分类器使用**单层决策树（decision stump），也称决策树桩**，它是一种简单的决策树，通过给定的阈值，进行分类。\n",
    "\n",
    "**1、数据集可视化**\n",
    "\n",
    "为了训练单层决策树，我们需要创建一个训练集，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#在下面添加此代码以显示单元格中的所有输出\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)                                             #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)                                         #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])        #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1])     #负样本散点图\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataArr,classLabels = loadSimpData()\n",
    "    showDataSet(dataArr,classLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，如果想要试着从某个坐标轴上选择一个值（即选择一条与坐标轴平行的直线）来将所有的蓝色圆点和橘色圆点分开，这显然是不可能的。这就是单层决策树难以处理的一个著名问题。通过使用多颗单层决策树，我们可以构建出一个能够对该数据集完全正确分类的分类器。\n",
    "\n",
    "**2、构建单层决策树**\n",
    "\n",
    "我们设置一个分类阈值，比如我横向切分，如下图所示：![jupyter](./AdaBoost-7.png)蓝横线上边的是一个类别，蓝横线下边是一个类别。显然，此时有一个蓝点分类错误，计算此时的分类误差，误差为1/5 = 0.2。这个横线与坐标轴的y轴的交点，就是我们设置的阈值，通过不断改变阈值的大小，找到使单层决策树的分类误差最小的阈值。同理，竖线也是如此，找到最佳分类的阈值，就找到了最佳单层决策树，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.400\n",
      "bestStump:\n",
      " {'dim': 0, 'thresh': 1.3, 'ineq': 'lt'}\n",
      "minError:\n",
      " [[0.2]]\n",
      "bestClasEst:\n",
      " [[-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    " \n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    "    \n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = float('inf')                                                        #最小误差初始化为正无穷大\n",
    "    for i in range(n):                                                            #遍历所有特征\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        #找到特征中最小的值和最大值\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                #计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:                                          #大于和小于的情况，均遍历。lt:less than，gt:greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataArr,classLabels = loadSimpData()\n",
    "    D = np.mat(np.ones((5, 1)) / 5)\n",
    "    bestStump,minError,bestClasEst = buildStump(dataArr,classLabels,D)\n",
    "    print('bestStump:\\n', bestStump)\n",
    "    print('minError:\\n', minError)\n",
    "    print('bestClasEst:\\n', bestClasEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码不难理解，就是通过遍历，改变不同的阈值，计算最终的分类误差，找到分类误差最小的分类方式，即为我们要找的最佳单层决策树。\n",
    "\n",
    "这里lt表示less than，表示分类方式，对于小于阈值的样本点赋值为-1，gt表示greater than，也是表示分类方式，对于大于阈值的样本点赋值为-1。\n",
    "\n",
    "经过遍历，我们找到，训练好的最佳单层决策树的最小分类误差为0.2，就是对于该数据集，无论用什么样的单层决策树，分类误差最小就是0.2。这就是我们训练好的弱分类器。\n",
    "\n",
    "接下来，使用AdaBoost算法提升分类器性能，将分类误差缩短到0，看下AdaBoost算法是如何实现的。\n",
    "\n",
    "# 使用AdaBoost提升分类器性能\n",
    "根据之前介绍的AdaBoost算法实现过程，使用AdaBoost算法提升分类器性能，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.400\n",
      "D: [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.250\n",
      "D: [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.143\n",
      "D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0\n",
      "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453}, {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565}, {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]\n",
      "[[ 1.17568763]\n",
      " [ 2.56198199]\n",
      " [-0.77022252]\n",
      " [-0.77022252]\n",
      " [ 0.61607184]]\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    "    \n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = float('inf')                                                        #最小误差初始化为正无穷大\n",
    "    for i in range(n):                                                            #遍历所有特征\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        #找到特征中最小的值和最大值\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                #计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:                                          #大于和小于的情况，均遍历。lt:less than，gt:greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    "\n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 20):\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)                                            #初始化权重\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)     #构建单层决策树\n",
    "        print(\"D:\",D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))         #计算弱学习算法权重alpha,使error不等于0,因为分母不能为0\n",
    "        bestStump['alpha'] = alpha                                          #存储弱学习算法权重\n",
    "        weakClassArr.append(bestStump)                                      #存储单层决策树\n",
    "        print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)     #计算e的指数项\n",
    "        D = np.multiply(D, np.exp(expon))                                      \n",
    "        D = D / D.sum()                                                        #根据样本权重公式，更新样本权重\n",
    "        #计算AdaBoost误差，当误差为0的时候，退出循环\n",
    "        aggClassEst += alpha * classEst                                 \n",
    "        print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m,1)))     #计算误差\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0: break                                             #误差为0，退出循环\n",
    "    return weakClassArr, aggClassEst\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataArr,classLabels = loadSimpData()\n",
    "    weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, classLabels)\n",
    "    print(weakClassArr)\n",
    "    print(aggClassEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一轮迭代中，D中的所有值都相等。于是，只有第一个数据点被错分了。因此在第二轮迭代中，D向量给第一个数据点0.5的权重。这就可以通过变量aggClassEst的符号来了解总的类别。第二次迭代之后，我们就会发现第一个数据点已经正确分类了，但此时最后一个数据点却是错分了。D向量中的最后一个元素变为0.5，而D向量中的其他值都变得非常小。最后，第三次迭代之后aggClassEst所有值的符号和真是类别标签都完全吻合，那么训练错误率为0，程序终止运行。\n",
    "\n",
    "最后训练结果包含了三个弱分类器，其中包含了分类所需要的所有信息。一共迭代了3次，所以训练了3个弱分类器构成一个使用AdaBoost算法优化过的分类器，分类器的错误率为0。\n",
    "\n",
    "一旦拥有了多个弱分类器以及其对应的alpha值，进行测试就变得想当容易了。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718]\n",
      " [ 0.69314718]]\n",
      "[[-1.66610226]\n",
      " [ 1.66610226]]\n",
      "[[-2.56198199]\n",
      " [ 2.56198199]]\n",
      "[[-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)                                             #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)                                         #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])        #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1])     #负样本散点图\n",
    "    plt.show()\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    "    \n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = float('inf')                                                        #最小误差初始化为正无穷大\n",
    "    for i in range(n):                                                            #遍历所有特征\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        #找到特征中最小的值和最大值\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                #计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:                                          #大于和小于的情况，均遍历。lt:less than，gt:greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                # print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    \"\"\"\n",
    "    使用AdaBoost算法提升弱分类器性能\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        numIt - 最大迭代次数\n",
    "    Returns:\n",
    "        weakClassArr - 训练好的分类器\n",
    "        aggClassEst - 类别估计累计值\n",
    "    \"\"\"\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)                                            #初始化权重\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)     #构建单层决策树\n",
    "        # print(\"D:\",D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))         #计算弱学习算法权重alpha,使error不等于0,因为分母不能为0\n",
    "        bestStump['alpha'] = alpha                                          #存储弱学习算法权重\n",
    "        weakClassArr.append(bestStump)                                      #存储单层决策树\n",
    "        # print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)     #计算e的指数项\n",
    "        D = np.multiply(D, np.exp(expon))                                      \n",
    "        D = D / D.sum()                                                        #根据样本权重公式，更新样本权重\n",
    "        #计算AdaBoost误差，当误差为0的时候，退出循环\n",
    "        aggClassEst += alpha * classEst                                      #计算类别估计累计值                                \n",
    "        # print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m,1)))     #计算误差\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        # print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0: break                                             #误差为0，退出循环\n",
    "    return weakClassArr, aggClassEst\n",
    "def adaClassify(datToClass,classifierArr):\n",
    "    \"\"\"\n",
    "    AdaBoost分类函数\n",
    "    Parameters:\n",
    "        datToClass - 待分类样例\n",
    "        classifierArr - 训练好的分类器\n",
    "    Returns:\n",
    "        分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(datToClass)\n",
    "    m = np.shape(dataMatrix)[0]\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):                                        #遍历所有分类器，进行分类\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])            \n",
    "        aggClassEst += classifierArr[i]['alpha'] * classEst\n",
    "        print(aggClassEst)\n",
    "    return np.sign(aggClassEst)\n",
    "if __name__ == '__main__':\n",
    "    dataArr,classLabels = loadSimpData()\n",
    "    weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, classLabels)\n",
    "    print(adaClassify([[0,0],[5,5]], weakClassArr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码很简单，在之前代码的基础上，添加adaClassify()函数，该函数遍历所有训练得到的弱分类器，利用单层决策树，输出的类别估计值乘以该单层决策树的分类器权重alpha，然后累加到aggClassEst上，最后通过sign函数最终的结果。可以看到，分类没有问题，(5,5)属于正类，(0,0)属于负类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在一个难数据集上应用AdaBoost\n",
    "Logistic回归实战篇之预测病马死亡率》文章中，我们使用Logistic回归方法训练马疝病数据集，预测病马死亡率。这个是使用Sklearn的LogisticRegression()训练的分类器，可以看到，正确率约为73.134%，也就是说错误率约为26.866%。可以看到错误率还是蛮高的，现在我们使用AdaBoost算法，训练出一个更强的分类器，这里的数据集有所变化，之前的标签是0和1，现在将标签改为+1和-1，其他数据不变。\n",
    "\n",
    "## 自己动手丰衣足食\n",
    "使用自己的用Python写的AbaBoost算法进行训练，添加loadDataSet函数用于加载数据集。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dim': 9, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.4616623792657674}, {'dim': 17, 'thresh': 52.5, 'ineq': 'gt', 'alpha': 0.31248245042467104}, {'dim': 3, 'thresh': 55.199999999999996, 'ineq': 'gt', 'alpha': 0.28680973201695786}, {'dim': 18, 'thresh': 62.300000000000004, 'ineq': 'lt', 'alpha': 0.23297004638939514}, {'dim': 10, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.19803846151213766}, {'dim': 5, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.18847887349020634}, {'dim': 12, 'thresh': 1.2, 'ineq': 'lt', 'alpha': 0.15227368997476795}, {'dim': 7, 'thresh': 1.2, 'ineq': 'gt', 'alpha': 0.15510870821690512}, {'dim': 5, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.13536197353359405}, {'dim': 4, 'thresh': 28.799999999999997, 'ineq': 'lt', 'alpha': 0.12521587326132078}, {'dim': 11, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.1334764812820768}, {'dim': 9, 'thresh': 4.0, 'ineq': 'lt', 'alpha': 0.1418224325377107}, {'dim': 14, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.10264268449708046}, {'dim': 0, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.11883732872109484}, {'dim': 4, 'thresh': 19.2, 'ineq': 'gt', 'alpha': 0.09879216527106643}, {'dim': 2, 'thresh': 36.72, 'ineq': 'lt', 'alpha': 0.12029960885056877}, {'dim': 3, 'thresh': 92.0, 'ineq': 'lt', 'alpha': 0.10846927663989193}, {'dim': 15, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.09652967982091411}, {'dim': 3, 'thresh': 73.6, 'ineq': 'gt', 'alpha': 0.08958515309272022}, {'dim': 18, 'thresh': 8.9, 'ineq': 'lt', 'alpha': 0.09210361961272426}, {'dim': 16, 'thresh': 4.0, 'ineq': 'gt', 'alpha': 0.10464142217079622}, {'dim': 11, 'thresh': 3.2, 'ineq': 'lt', 'alpha': 0.09575457291711606}, {'dim': 20, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.09624217440331542}, {'dim': 17, 'thresh': 37.5, 'ineq': 'lt', 'alpha': 0.0785966288518967}, {'dim': 9, 'thresh': 2.0, 'ineq': 'lt', 'alpha': 0.071428636345507}, {'dim': 5, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.07830753154662214}, {'dim': 4, 'thresh': 28.799999999999997, 'ineq': 'lt', 'alpha': 0.07606159074712804}, {'dim': 4, 'thresh': 19.2, 'ineq': 'gt', 'alpha': 0.08306752811081955}, {'dim': 7, 'thresh': 4.2, 'ineq': 'gt', 'alpha': 0.0830416741141177}, {'dim': 3, 'thresh': 92.0, 'ineq': 'lt', 'alpha': 0.08893356802801204}, {'dim': 14, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.07000509315417898}, {'dim': 7, 'thresh': 5.3999999999999995, 'ineq': 'lt', 'alpha': 0.07697582358565984}, {'dim': 18, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.08507457442866707}, {'dim': 5, 'thresh': 3.2, 'ineq': 'lt', 'alpha': 0.0676590387302067}, {'dim': 7, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.08045680822237039}, {'dim': 12, 'thresh': 1.2, 'ineq': 'lt', 'alpha': 0.05616862921969537}, {'dim': 11, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.06454264376249873}, {'dim': 7, 'thresh': 5.3999999999999995, 'ineq': 'lt', 'alpha': 0.053088884353828844}, {'dim': 11, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.0734605861478849}, {'dim': 13, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.07872267320907414}]\n",
      "训练集的错误率:19.732%\n",
      "测试集的错误率:19.403%\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len((open(fileName).readline().split('\\t')))\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat - 1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat, labelMat\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    "    \n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = float('inf')                                                        #最小误差初始化为正无穷大\n",
    "    for i in range(n):                                                            #遍历所有特征\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        #找到特征中最小的值和最大值\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                #计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:                                          #大于和小于的情况，均遍历。lt:less than，gt:greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                # print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    " \n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    \"\"\"\n",
    "    使用AdaBoost算法提升弱分类器性能\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        numIt - 最大迭代次数\n",
    "    Returns:\n",
    "        weakClassArr - 训练好的分类器\n",
    "        aggClassEst - 类别估计累计值\n",
    "    \"\"\"\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)                                            #初始化权重\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)     #构建单层决策树\n",
    "        # print(\"D:\",D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))         #计算弱学习算法权重alpha,使error不等于0,因为分母不能为0\n",
    "        bestStump['alpha'] = alpha                                          #存储弱学习算法权重\n",
    "        weakClassArr.append(bestStump)                                      #存储单层决策树\n",
    "        # print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)     #计算e的指数项\n",
    "        D = np.multiply(D, np.exp(expon))                                      \n",
    "        D = D / D.sum()                                                        #根据样本权重公式，更新样本权重\n",
    "        #计算AdaBoost误差，当误差为0的时候，退出循环\n",
    "        aggClassEst += alpha * classEst                                      #计算类别估计累计值                                \n",
    "        # print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m,1)))     #计算误差\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        # print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0: break                                             #误差为0，退出循环\n",
    "    return weakClassArr, aggClassEst\n",
    " \n",
    "def adaClassify(datToClass,classifierArr):\n",
    "    \"\"\"\n",
    "    AdaBoost分类函数\n",
    "    Parameters:\n",
    "        datToClass - 待分类样例\n",
    "        classifierArr - 训练好的分类器\n",
    "    Returns:\n",
    "        分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(datToClass)\n",
    "    m = np.shape(dataMatrix)[0]\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):                                        #遍历所有分类器，进行分类\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])            \n",
    "        aggClassEst += classifierArr[i]['alpha'] * classEst\n",
    "        # print(aggClassEst)\n",
    "    return np.sign(aggClassEst)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataArr, LabelArr = loadDataSet('horseColicTraining2.txt')\n",
    "    weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, LabelArr)\n",
    "    testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "    print(weakClassArr)\n",
    "    predictions = adaClassify(dataArr, weakClassArr)\n",
    "    errArr = np.mat(np.ones((len(dataArr), 1)))\n",
    "    print('训练集的错误率:%.3f%%' % float(errArr[predictions != np.mat(LabelArr).T].sum() / len(dataArr) * 100))\n",
    "    predictions = adaClassify(testArr, weakClassArr)\n",
    "    errArr = np.mat(np.ones((len(testArr), 1)))\n",
    "    print('测试集的错误率:%.3f%%' % float(errArr[predictions != np.mat(testLabelArr).T].sum() / len(testArr) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里输出了AdaBoost算法训练好的分类器的组合，我们只迭代了40次，也就是训练了40个弱分类器。\n",
    "\n",
    "最终，训练集的错误率为19.732%，测试集的错误率为19.403%，可以看到相对于Sklearn的罗辑回归方法，错误率降低了很多。\n",
    "\n",
    "这个仅仅是我们训练40个弱分类器的结果，如果训练更多弱分类器，效果会更好。但是当弱分类器数量过多的时候，你会发现训练集错误率降低很多，但是测试集错误率提升了很多，这种现象就是**过拟合(overfitting)**。分类器对训练集的拟合效果好，但是缺失了普适性，只对训练集的分类效果好，这是我们不希望看到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Sklearn的AdaBoost\n",
    "\n",
    "sklearn.ensemble模块提供了很多集成方法，**AdaBoost、Bagging、随机森林等。本文使用的是AdaBoostClassifier**。参考：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "![jupyter](./adaboost-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参数说明如下：**\n",
    "**base_estimator**：可选参数，默认为DecisionTreeClassifier。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。\n",
    "\n",
    "**algorithm**：可选参数，默认为SAMME.R。scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "**n_estimators**：整数型，可选参数，默认为50。弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "**learning_rate**：浮点型，可选参数，默认为1.0。每个弱学习器的权重缩减系数，取值范围为0到1，对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1。\n",
    "\n",
    "**random_state**：整数型，可选参数，默认为None。如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。\n",
    "\n",
    "了解这些，我们就可以开始编写代码了。完成上述代码相似的功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的错误率:16.054%\n",
      "测试集的错误率:17.910%\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-11\n",
    "\"\"\"\n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len((open(fileName).readline().split('\\t')))\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat - 1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat, labelMat\n",
    "if __name__ == '__main__':\n",
    "    dataArr, classLabels = loadDataSet('horseColicTraining2.txt')\n",
    "    testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), algorithm = \"SAMME\", n_estimators = 10)\n",
    "    bdt.fit(dataArr, classLabels)\n",
    "    predictions = bdt.predict(dataArr)\n",
    "    errArr = np.mat(np.ones((len(dataArr), 1)))\n",
    "    print('训练集的错误率:%.3f%%' % float(errArr[predictions != classLabels].sum() / len(dataArr) * 100))\n",
    "    predictions = bdt.predict(testArr)\n",
    "    errArr = np.mat(np.ones((len(testArr), 1)))\n",
    "    print('测试集的错误率:%.3f%%' % float(errArr[predictions != testLabelArr].sum() / len(testArr) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用**DecisionTreeClassifier**作为使用的弱分类器，使用AdaBoost算法训练分类器。可以看到训练集的错误率为16.054%，测试集的错误率为：17.910%。更改n_estimators参数，你会发现跟我们自己写的代码，更改迭代次数的效果是一样的。n_enstimators参数过大，会导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类器性能评价\n",
    "之前讲了很多分类器。我们都是假设所有类别的分类代价是一样的。比如在逻辑回归那篇文章中，我们构建了一个用于检测患疝病的马匹是否存活的系统。在那里，我们构建了分类器，但是并没有对分类后的情形加以讨论。假如某人给我们牵来一匹马，他希望我们能预测这匹马能否生存。我们说马会死，那么他们就可能会对马实施安乐死，而不是通过给马喂药来延缓其不可避免的死亡过程。我们的预测也许是错误的，马本来是可以继续活着的。毕竟，我们的分类器只有80%的精确率（accuracy）。如果我们预测错误，那么我们将会错杀一个如此昂贵的动物，更不要说人对马还存在情感上的依恋了。\n",
    "\n",
    "再比如，如何过滤垃圾邮件呢？如果收件箱中会出现某些垃圾邮件，但合法邮件永远不会扔进垃圾邮件夹中，人们会是否会满意呢？显然，我们可以忍受收件箱中偶尔出现的垃圾邮件，但是绝不能忍受，合法邮件被误扔如垃圾邮件夹中，万一这是一封女神or男神的表白信，这岂不是因此错过了一段旷世姻缘？\n",
    "\n",
    "很多时候，不同类别的分类代价并不相等，这就是**非均衡分类问题**。我们将会考察一种新的分类器性能度量方法，而不再是简单的通过**错误率**进行评价，并且通过图像技术来对上述非均衡问题下不同分类器性能进行可视化处理。\n",
    "\n",
    "## 分类器性能度量指标\n",
    "在之前，我们都是基于**错误率**来衡量分类器任务的成功程度的。错误率指的是在所有测试样本中错分的样本比例。实际上，这样的度量错误掩盖了样例如何被错分的事实。在机器学习中，有一个普遍适用的称为**混淆矩阵**（confusion matrix）的工具，它可以帮助人们更好地了解分类中的错误。有这样一个关于在房子周围可能发现的动物类型的预测，这个预测的三个类问题的混淆矩阵如下图所示：![jupyter](./adaboost-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用混淆矩阵就可以更好地理解分类中的错误了。如果矩阵中的非对角元素均为0，就会得到一个完美的分类器。\n",
    "\n",
    "接下来，我们考虑另外一个混淆矩阵，这次的矩阵只针对一个简单的二类问题。混淆矩阵如下图所示：![jupyter](./adaboost-10.png)可以看到，在这个二分类问题中，如果对一个正例正确地判为正例，那么就可以认为产生了一个**真正例**（True Positive，TP，也称真阳）；如果对一个反例正确地判为反例，则认为产生了一个**真反例**（True Negative，TN，也称真阴）；如果对一个正例错误地判为反例，那么就可以认为产生了一个**伪反例**（False Negative，FN，为称假阴）；如果对一个反例错误地判为正例，则认为产生了一个**伪正例**（False Positive，FP，也称假阳）。\n",
    "\n",
    "在分类中，当某个类别的重要性高于其他类别时，我们就可以来利用上述定义来定义出多个比错误率更好的指标。从混淆矩阵中，可以衍生出各种评价指标。如下图(来自wiki)所示：![jupyter](./adaboost-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个指标的定义及含义如下：\n",
    "**（1）Accuracy模型的精度**，即模型预测正确的个数/样本的总个数$Accuracy=\\frac{TP+TN}{TP+FN+FP+TN}$一般情况下，模型的精度越高，说明模型的效果越好。\n",
    "\n",
    "**（2）Positive predictive value（PPV，Precision）正确率**，阳性预测值，在模型预测为正类的样本中，真正的正样本所占的比例。$Precision=\\frac{TP}{TP+FP}$一般情况下，正确率越高，说明模型的效果越好。\n",
    "\n",
    "**（3）False discovery rate（FDR）伪发现率**，也是错误发现率，表示在模型预测为正类的样本中，真正的负类的样本所占的比例。$FDR=\\frac{FP}{TP+FP}$一般情况下，错误发现率越小，说明模型的效果越好。\n",
    "\n",
    "**（4）False omission rate（FOR）错误遗漏率**，表示在模型预测为负类的样本中，真正的正类所占的比例。即评价模型\"遗漏\"掉的正类的多少。$FOR=\\frac{FN}{FN+TN}$\n",
    "\n",
    "**（5）Negative predictive value（NPV）阴性预测值**，在模型预测为负类的样本中，真正为负类的样本所占的比例。$NPV=\\frac{TN}{FN+TN}$一般情况下，NPV越高，说明的模型的效果越好。\n",
    "\n",
    "**（6）True positive rate（TPR，Recall）召回率**，真正类率，表示的是，模型预测为正类的样本的数量，占总的正类样本数量的比值。$Recall=\\frac{TP}{FN+TP}$一般情况下，Recall越高，说明有更多的正类样本被模型预测正确，模型的效果越好。\n",
    "\n",
    "**（7）False positive rate（FPR），Fall-out假正率**，表示的是模型预测为正类的样本中，占模型负类样本数量的比值。$FPR=\\frac{FP}{FP+TN}$一般情况下，假正类率越低，说明模型的效果越好。\n",
    "\n",
    "**（8）False negative rate（FNR），Miss rate假负类率，缺失率**，模型预测为负类的样本中，是正类的数量，占真实正类样本的比值。$FNR=\\frac{FN}{FN+TN}$缺失值越小，说明模型的效果越好。\n",
    "\n",
    "公式有些多？有些眼花撩乱，没关系，慢慢理解就好了。我们可以很容易构造一个高正确率或高召回率的分类器，但是很难同时保证两者成立。如果将任何样本都判为正例，那么召回率达到百分之百而此时正确率很低。构建一个同时使正确率和召回率最大的分类器是具有挑战性的。\n",
    "\n",
    "除了上述的评价指标，另一个用于度量分类中的**非均衡的工具是ROC曲线（ROC curve）**，ROC代表接收者操作特征（receiver operating characteristic），它最早在二战期间由电气工程师构建雷达系统时使用过。\n",
    "\n",
    "先运行个程序，我们看下结果，再听我细细道来，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC面积为: 0.8582969635063604\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    " \n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-10\n",
    "\"\"\"\n",
    " \n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len((open(fileName).readline().split('\\t')))\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat - 1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    " \n",
    "    return dataMat, labelMat\n",
    " \n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    " \n",
    " \n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = float('inf')                                                        #最小误差初始化为正无穷大\n",
    "    for i in range(n):                                                            #遍历所有特征\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        #找到特征中最小的值和最大值\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                #计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:                                          #大于和小于的情况，均遍历。lt:less than，gt:greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                # print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    " \n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    \"\"\"\n",
    "    使用AdaBoost算法训练分类器\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        numIt - 最大迭代次数\n",
    "    Returns:\n",
    "        weakClassArr - 训练好的分类器\n",
    "        aggClassEst - 类别估计累计值\n",
    "    \"\"\"\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)                                            #初始化权重\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)     #构建单层决策树\n",
    "        # print(\"D:\",D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))         #计算弱学习算法权重alpha,使error不等于0,因为分母不能为0\n",
    "        bestStump['alpha'] = alpha                                          #存储弱学习算法权重\n",
    "        weakClassArr.append(bestStump)                                      #存储单层决策树\n",
    "        # print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)     #计算e的指数项\n",
    "        D = np.multiply(D, np.exp(expon))                                     \n",
    "        D = D / D.sum()                                                        #根据样本权重公式，更新样本权重\n",
    "        #计算AdaBoost误差，当误差为0的时候，退出循环\n",
    "        aggClassEst += alpha * classEst                                      #计算类别估计累计值                               \n",
    "        # print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m,1)))     #计算误差\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        # print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0: break                                             #误差为0，退出循环\n",
    "    return weakClassArr, aggClassEst\n",
    " \n",
    " \n",
    "def plotROC(predStrengths, classLabels):\n",
    "    \"\"\"\n",
    "    绘制ROC\n",
    "    Parameters:\n",
    "        predStrengths - 分类器的预测强度\n",
    "        classLabels - 类别\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)\n",
    "    cur = (1.0, 1.0)                                                         #绘制光标的位置\n",
    "    ySum = 0.0                                                                 #用于计算AUC\n",
    "    numPosClas = np.sum(np.array(classLabels) == 1.0)                        #统计正类的数量\n",
    "    yStep = 1 / float(numPosClas)                                             #y轴步长   \n",
    "    xStep = 1 / float(len(classLabels) - numPosClas)                         #x轴步长\n",
    " \n",
    "    sortedIndicies = predStrengths.argsort()                                 #预测强度排序\n",
    " \n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index] == 1.0:\n",
    "            delX = 0; delY = yStep\n",
    "        else:\n",
    "            delX = xStep; delY = 0\n",
    "            ySum += cur[1]                                                     #高度累加\n",
    "        ax.plot([cur[0], cur[0] - delX], [cur[1], cur[1] - delY], c = 'b')     #绘制ROC\n",
    "        cur = (cur[0] - delX, cur[1] - delY)                                 #更新绘制光标的位置\n",
    "    ax.plot([0,1], [0,1], 'b--')\n",
    "    plt.title('AdaBoost马疝病检测系统的ROC曲线', FontProperties = font)\n",
    "    plt.xlabel('假阳率', FontProperties = font)\n",
    "    plt.ylabel('真阳率', FontProperties = font)\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "    print('AUC面积为:', ySum * xStep)                                         #计算AUC\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataArr, LabelArr = loadDataSet('horseColicTraining2.txt')\n",
    "    weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, LabelArr, 10)\n",
    "    plotROC(aggClassEst.T, LabelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到有两个输出结果，一个是AUC面积，另一个ROC曲线图。\n",
    "\n",
    "解释ROC，图中的横坐标是伪正例的比例（假阳率=FP/（FP+TN）），而纵坐标是真正例的比例（真阳率=TP/（TP+FN））。ROC曲线给出的是当阈值变化时假阳率和真阳率的变化情况。左下角的点所对应的将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判为正例的情况。虚线给出的是随机猜测的结果曲线。\n",
    "\n",
    "ROC曲线不但可以用于比较分类器，还可以基于成本效益（cost-versus-benefit）分析来做出决策。由于在不同的阈值下，不用的分类器的表现情况是可能各不相同，因此以某种方式将它们组合起来或许更有意义。如果只是简单地观察分类器的错误率，那么我们就难以得到这种更深入的洞察效果了。\n",
    "\n",
    "在理想的情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。例如在垃圾邮件的过滤中，就相当于过滤了所有的垃圾邮件，但没有将任何合法邮件误识别为垃圾邮件而放入垃圾邮件额文件夹中。\n",
    "\n",
    "对不同的ROC曲线进行比较的一个指标是曲线下的面积（Area Unser the Curve，AUC）。AUC给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。一个完美分类器的ACU为1.0，而随机猜测的AUC则为0.5。\n",
    "\n",
    "这个ROC曲线是怎么画的呢？\n",
    "\n",
    "对于分类器而言，都有概率输出的功能，拿逻辑回归来举例，我们得到的是该样本属于正样本的概率和属于负样本的概率，属于正样本的概率大，那么就判为正类，否则判为负类，那么实质上这里的阈值是0.5。\n",
    "\n",
    "# 总结\n",
    "AdaBoost的优缺点：\n",
    "\n",
    "优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。\n",
    "\n",
    "缺点：对离群点敏感。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
