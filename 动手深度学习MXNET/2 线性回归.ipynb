{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为机器学习的⼀类，深度学习通常基于**神经⽹络**模型逐级表⽰越来越抽象的概念或模式。我们先从线性回归和 softmax 回归这两种单层神经⽹络⼊⼿，简要介绍机器学习中的基本概念。然后，由单层神经⽹络延伸到多层神经⽹络，并通过多层感知机引⼊深度学习模型。在观察和了解了模型的**过拟合现象**后，我们将介绍深度学习中应对过拟合的常⽤⽅法：**权重衰减和丢弃法**。接着，为了进⼀步理解深度学习模型训练的本质，我们将详细解释**正向传播和反向传播**。掌握这两个概念后，我们能更好地认识深度学习中的**数值稳定性和初始化**的⼀些问题。最后，我们通过⼀个深度学习应⽤案例对本章内容学以致⽤。\n",
    "\n",
    "我们先介绍单层神经⽹络：线性回归和softmax回归。\n",
    "\n",
    "**线性回归**输出是⼀个连续值，因此适⽤于回归问题。回归问题在实际中很常⻅，如预测房屋价格、⽓温、销售额等连续值的问题。与回归问题不同，**分类问题**中模型的最终输出是⼀个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适⽤于分类问题。\n",
    "\n",
    "由于线性回归和softmax回归都是单层神经⽹络，它们涉及的概念和技术同样适⽤于⼤多数的深度学习模型。我们⾸先以线性回归为例，介绍⼤多数深度学习模型的基本要素和表⽰⽅法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有如下简单**线性模型**$$\\hat y = x_1w_1 + x_2w_2 + b$$其中 $w_1$, $w_2$ 是权重（weight），通常⽤向量 $w = [w_1, w_2]^⊤$来表⽰；b是偏差（bias）。这⾥的权重和偏差是线性回归模型的参数（parameter）。\n",
    "\n",
    "在应用中，可以假设通过房屋的⾯积（设 $x_1$）和房龄（设 $x_2$）来**估算**它的真实价格（设 y）。接下来，让我们了解⼀下如何通过训练模型来学习模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习模型需要**训练数据**。房屋数据集中，⼀栋房屋的特征和标签就是⼀个数据样本。设训练数据集样本数为 n，索引为 i 样本的特征为$x^{(i)}_1,x^{(i)}_2$，标签为$ y^{(i)}$。对于索引为 i 的房屋，线性回归模型的价格估算表达式为$$\\hat y^{(i)} = x^{(i)}_1 w_1 + x^{(i)}_2w_2 + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型训练中，希望模型的估计值和真实值在训练数据集上尽可能接近。⽤平⽅损失（square loss）来定义数据样本 i 上的**损失**（loss）为$$\\ell^{(i)}(w_1, w_2, b) =\\frac{ (\\hat y^{(i)}- y^{(i)})^2}{2}$$显然，当该损失越小时，模型在数据样本 i 上的估计值和真实值越接近。其中的常数$\\frac{1}{2}$将使得求导后常数为1，在形式上会简单些。\n",
    "\n",
    "已知训练数据集样本数为 n。线性回归的⽬标是找到**⼀组模型参数**$w_1, w_2, b$ 来最小化损失函数$$\\ell(w_1, w_2, b)=\\frac{1}{n}\\sum_{i=1}^n\\ell ^{(i)}(w_1, w_2, b)=\\frac{1}{n}\\sum_{i=1}^n\\frac{1}{2}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})^2$$\n",
    "在上式中，损失函数 $\\ell(w_1, w_2, b) $可看作是训练数据集中各个样本上损失的平均，平方误差函数也称为**平方损失**。\n",
    "\n",
    "在模型训练中，我们希望找出⼀组模型参数，记为$w^*_1, w^*_2, b^*$来使训练样本平均损失最小：$$w^*_1, w^*_2, b^* =argmin_{w_1,w_2,b}\\ell(w_1,w_2,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归这个简单例⼦⾥，令损失函数对模型参数求导后的结果为零可以解出最小化损失函数的模型参数。这类解叫做解析解（analytical solution）。然而，⼤多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫做**数值解（**numerical solution）。\n",
    "\n",
    "在求数值解的优化算法中，**小批量随机梯度下降（mini-batch stochastic gradient descent）**被⼴泛使⽤。它的算法很简单：每⼀次迭代前，我们可以随机均匀采样⼀个由训练数据样本所组成的小批量（mini-batch）$\\beta$；然后求小批量中数据样本的平均损失有关模型参数的导数（梯度）；再⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减小量。\n",
    "\n",
    "在训练本节讨论的线性回归模型的过程中，模型的每个参数将迭代如下：![jupyter](./LineReg-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上式中，$|\\beta|$代表每个小批量中的样本个数（批量⼤小，batch size），η 称作**学习率**（learningrate）并取正数。需要强调的是，这⾥的批量⼤小和学习率的值是⼈为设定的，并不是通过模型训练学出的，因此叫做**超参数**（hyperparameter）。我们通常所说的**“调参”**指的正是调节超参数，例如通过反复试错来找到合适的超参数。少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。\n",
    "\n",
    "我们将在之后的“优化算法”⼀章中详细解释小批量随机梯度下降和其他优化算法。\n",
    "\n",
    "得到模型参数后，就可以对任意输入计算估值，估算也叫作**模型预测**、模型推断或模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归的表⽰⽅法\n",
    "我们已经阐述了线性回归的模型表达式、训练和预测。下⾯我们解释**线性回归与神经⽹络的联系**，以及线性回归的⽮量计算表达式。\n",
    "\n",
    "**神经⽹络图**\n",
    "\n",
    "在深度学习中，我们可以使⽤神经⽹络图直观地表现模型结构。为了更清晰地展⽰线性回归作为神经⽹络的结构，下图使⽤神经⽹络图表⽰本节中介绍的线性![jupyter](./LineReg-2.png)回归模型。神经⽹络图隐去了模型参数权重和偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输⼊分别为$x_1$和$x_2$，因此输⼊层的输⼊个数为2。输⼊个数也叫特征数或**特征向量维度**。图中⽹络的输出为o，输出层的输出个数为1。需要注意的是，我们直接将图中神经⽹络的输出o作为线性回归的输出，即$\\hat y = o$。由于输⼊层并不涉及计算，按照惯例，神经⽹络的层数为1。所以，**线性回归是⼀个单层神经⽹络**。输出层中负责计算o的单元⼜叫神经元。在线性回归中，o的计算依赖于x1和x2。也就是说，输出层中的神经元和输⼊层中各个输⼊完全连接。因此，这⾥的输出层⼜叫全连接层（fully-connected layer）或稠密层（dense layer）。\n",
    "\n",
    "**⽮量计算表达式**\n",
    "\n",
    "在模型训练或预测时，我们常常会同时处理多个数据样本并⽤到⽮量计算。在介绍线性回归的⽮量计算表达式之前，让我们先考虑对两个向量相加的两种⽅法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from time import time\n",
    "a = nd.ones(shape=1000) \n",
    "b = nd.ones(shape=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6319973468780518"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#向量相加的⼀种⽅法是，将这两个向量按元素逐⼀做标量加法。\n",
    "start = time()\n",
    "c = nd.zeros(shape=1000)\n",
    "for i in range(1000):\n",
    "    c[i] = a[i] + b[i]\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009984970092773438"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将两个向量直接做矢量加法\n",
    "start = time()\n",
    "d=a+b\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后者⽐前者更省时。因此，我们应该**尽可能采⽤⽮量计算**，以提升计算效率。\n",
    "\n",
    "对训练数据集⾥的3个房屋样本（索引分别为1、 2和3）逐⼀预测价格，将得到![jupyter](./LineReg-3.png)将上⾯3个等式转化成⽮量计算。设![jupyter](./LineReg-4.png)\n",
    "对3个房屋样本预测价格的⽮量计算表达式为$\\hat y = Xw + b$, 其中的加法运算使⽤了⼴播机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⼴义上讲，当数据样本数为n，特征数为d时，线性回归的⽮量计算表达式为$\\hat y = Xw + b$\n",
    "\n",
    "设模型参数$\\theta=[w_1,w_2,b]^T$，重写损失函数$$\\ell(\\theta)=\\frac{1}{2n}(\\hat y-y)^T(\\hat y-y)$$小批量随机梯度下降的迭代步骤将相应地改写为$$\\theta \\longleftarrow \\theta-\\frac{\\eta}{|\\beta|}\\sum_{i\\in \\beta}\\bigtriangledown _\\theta \\ell^{(i)}(\\theta)$$其中梯度是损失有关3个为标量的模型参数的偏导数组成的向量：![jupyter](./LineReg-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归从零实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在了解了线性回归的背景知识之后，现在我们可以动⼿实现它了。尽管强⼤的深度学习框架可以减少⼤量重复性⼯作，但若过于依赖它提供的便利，我们就会很难深⼊理解深度学习是如何⼯作的。因此，本节将介绍如何**只利⽤NDArray和autograd**来实现⼀个线性回归的训练。⾸先，导⼊本节中实验所需的包或模块，其中的matplotlib包⽤于作图。\n",
    "\n",
    "我们在这⾥描述⽤来⽣成⼈⼯训练数据集的真实模型。使⽤⼈⼯训练数据集将使我们能够⽐较学到的参数和真实的模型参数。设训练数据集样本数为1000，输⼊个数为2。给定随机⽣成的批量样本特征$X∈R^{1000×2}$，我们使⽤线性回归模型真实权重$w=[2,−3.4]^⊤$和偏差b=4.2，以及⼀个随机噪⾳项ϵ来⽣成标签。\n",
    "$y=Xw+b+\\epsilon$\n",
    "\n",
    "其中噪⾳项ϵ服从均值为0和标准差为0.01的正态分布。下⾯，让我们⽣成数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import set_matplotlib_formats\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成训练集\n",
    "num_inputs=2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "#均值loc,标准差scale\n",
    "features = nd.random.normal(loc=0,scale=1, shape=(num_examples, num_inputs))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "def set_figsize(figsize=(3.5,2.5)):\n",
    "    set_matplotlib_formats('svg')#打印矢量图\n",
    "    plt.rcParams['figure.figsize']=figsize#设置尺寸\n",
    "    \n",
    "set_figsize()\n",
    "'''\n",
    "plt.scatter(features[:,1].asnumpy(),labels.asnumpy(),1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个函数：它每次返回batch_size（批量大小）个随机样本的特征和标签。\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  #shuffle() 方法将序列的所有元素随机排序\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)  # take函数根据索引返回对应元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-1.4069158   0.2274378 ]\n",
      " [-0.5732961  -1.0055317 ]\n",
      " [ 0.52791333 -1.0662739 ]\n",
      " [-1.0647175   0.48607218]\n",
      " [ 0.18270281 -1.8266431 ]\n",
      " [ 0.80127084 -0.80003786]\n",
      " [-1.8189818   0.79729116]\n",
      " [-0.6585579   0.25549018]\n",
      " [ 0.43067265  0.59737974]\n",
      " [ 0.7151041  -0.02540994]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 0.62472105  6.4676166   8.892502    0.41963398 10.773921    8.5067835\n",
      " -2.1460445   2.0301354   3.0335236   5.7279325 ]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#读取第一个小批量数据样本并打印。每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小。\n",
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。\n",
    "w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "b = nd.zeros(shape=(1,))\n",
    "#模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们需要创建它们的梯度。\n",
    "w.attach_grad()#申请存储梯度所需要的内存。\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义模型、损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归的矢量计算表达式的实现。我们使用dot函数做矩阵乘法。\n",
    "def linreg(X, w, b):  \n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "#平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值y变形成预测值y_hat的形状。以下函数返回的结果也将和y_hat的形状相同。\n",
    "def squared_loss(y_hat, y):  \n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义优化算法**\n",
    "\n",
    "**sgd函数**实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型\n",
    "在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。由于我们之前设批量大小batch_size为10，每个小批量的损失l的形状为(10, 1)。回忆一下“自动求梯度”一节。由于变量l并不是一个标量，运行l.backward()将对l中元素求和得到新的变量，再求该变量有关模型参数的梯度。\n",
    "在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数num_epochs和学习率lr都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。我们会在后面“优化算法”一章中详细介绍学习率对模型的影响。\n",
    "\n",
    "**训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.042665\n",
      "epoch 2, loss 0.000167\n",
      "epoch 3, loss 0.000050\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03#学习率\n",
    "num_epochs = 3#迭代周期数\n",
    "net = linreg#网络\n",
    "loss = squared_loss#损失\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X\n",
    "    # 和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，我们可以比较学到的参数和用来生成训练集的真实参数。它们应该很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " \n",
       " [[ 1.9986786]\n",
       "  [-3.39897  ]]\n",
       " <NDArray 2x1 @cpu(0)>,\n",
       " 4.2,\n",
       " \n",
       " [4.198858]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w, w,true_b, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归的简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着深度学习框架的发展，开发深度学习应用变得越来越便利。实践中，我们通常可以用比上一节更简洁的代码来实现同样的模型。在本节中，我们将介绍如何**使用MXNet提供的Gluon接口**更方便地实现线性回归的训练。\n",
    "\n",
    "**生成数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)#加上一个噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gluon提供了data包来读取数据**。由于data常用作变量名，我们将导入的data模块用添加了Gluon首字母的假名gdata代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = gdata.ArrayDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里data_iter的使用与上一节中的一样。让我们读取并打印第一个小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.3697431   1.1665803 ]\n",
      " [ 1.170625    0.09366855]\n",
      " [ 2.8202667  -0.3467094 ]\n",
      " [-0.20904449 -1.3123679 ]\n",
      " [-0.52371514  1.5088465 ]\n",
      " [-0.4555083  -0.26751143]\n",
      " [ 0.04713194 -0.4263745 ]\n",
      " [-0.05043959 -1.4717678 ]\n",
      " [-0.9598788   0.5165809 ]\n",
      " [-0.20458293 -1.7121046 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 2.9866602  6.226853  11.023338   8.2451105 -2.0022683  4.204228\n",
      "  5.73937    9.100429   0.5309218  9.6020355]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义模型**\n",
    "\n",
    "在上一节从零开始的实现中，我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更烦琐。其实，Gluon提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用Gluon更简洁地定义线性回归。\n",
    "\n",
    "首先，导入nn模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该**模块定义了大量神经网络的层**。我们先定义一个模型变量net，它是一个Sequential实例。在Gluon中，**Sequential实例可以看作是一个串联各个层的容器**。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "net = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾图3.1中线性回归在神经网络图中的表示。作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。在Gluon中，**全连接层是一个Dense实例**。我们定义该层**输出个数为1**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型得到数据时，例如后面执行net(X)时，模型将自动推断出每一层的输入个数。我们将在之后“深度学习计算”一章详细介绍这种机制。Gluon的这一设计为模型开发带来便利。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**初始化模型参数**\n",
    "\n",
    "在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。我们从MXNet导入**init模块**。该模块提供了模型参数初始化的各种方法。这里的init是initializer的缩写形式。我们通过init.Normal(sigma=0.01)指定权重参数每个元素将在初始化时随机采样于**均值为0、标准差为0.01的正态分布**。偏差参数默认会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义损失函数**\n",
    "\n",
    "在Gluon中，**loss模块**定义了各种损失函数。我们用假名gloss代替导入的loss模块，并直接使用它提供的平方损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss\n",
    "loss = gloss.L2Loss()  # 平方损失又称L2范数损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义优化算法**\n",
    "\n",
    "同样，我们也无须实现小批量随机梯度下降。在导入Gluon后，我们**创建一个Trainer实例**，并指定学习率为0.03的小批量随机梯度下降（sgd）为优化算法。该优化算法将用来迭代net实例所有通过add函数嵌套的层所包含的全部参数。这些参数可以通过collect_params函数获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用**Gluon训练模型**时，我们通过调用Trainer实例的step函数来迭代模型参数。上一节中我们提到，由于变量l是长度为batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.044671\n",
      "epoch 2, loss: 0.000178\n",
      "epoch 3, loss: 0.000053\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们分别比较学到的模型参数和真实的模型参数。我们从net获得需要的层，并访问其**权重（weight）和偏差（bias）**。学到的参数和真实的参数很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " \n",
       " [[ 1.9994407 -3.3998003]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "true_w, dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2,\n",
       " \n",
       " [4.199752]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias.data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
